<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Baoqp&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Baoqp&#39;s Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Baoqp&#39;s Blog">
<meta property="og:locale">
<meta property="article:author" content="Bao Qingping">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Baoqp&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Baoqp&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Cassandra源码阅读随笔5-SELECT操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/23/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%945-SELECT%E6%93%8D%E4%BD%9C/" class="article-date">
  <time datetime="2019-09-23T01:10:02.000Z" itemprop="datePublished">2019-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/23/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%945-SELECT%E6%93%8D%E4%BD%9C/">Cassandra源码阅读随笔5. SELECT操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="select语句的解析"><a href="#select语句的解析" class="headerlink" title="select语句的解析"></a>select语句的解析</h2><p>在users表执行如下查询</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from users where id = 3;</span><br></pre></td></tr></table></figure>
<p>首先查询语句被解析为SelectStatement类的静态内部类RawStatement的实例，对于当前的语句，主要保存了要查询的表cdName和查询条件，其中查询条件用SingleColumnRelation表示。之后调用RawStatement的prepare()方法生成SelectStatement实例。</p>
<p>cassandra中查询条件的使用和mysql中的不同，会有更多的限制。比如不能直接使用clustering key进行查询，因为要查询所有分区，性能差，所以Cassandra默认不支持；同样也不能在普通列上面设置查询条件，只有在创建了二级索引之后才能使用，但是这样的话也会访问所有分区，所以建议根据要查询的关系再创建一个冗余的表。 </p>
<p>生成SelectStatement之后，调用SelectStatement的execute()方法，该方法中主要把查询相关的信息组成ReadCommand，其中包括要查询的keyspace, columnfamily, key等，之后依次调用StorageProxy的read()和fetchRows()方法。 fetchRows()方法如下，主要有以下几个步骤：</p>
<ol>
<li>为每个读取命令（ReadCommand）生成ReadExecutor，并异步执行</li>
<li>获取读取结果，如果出现DigestMismatchException，则要进行读取修复，进入步骤3，否则返回读取结果</li>
<li>向所有节点发送读取修复的写入请求，并等待写入完成，写入完成后判断是否要再次读取。</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">private static List&lt;Row&gt; fetchRows(List&lt;ReadCommand&gt; initialCommands, ConsistencyLevel consistencyLevel)</span><br><span class="line">        throws UnavailableException, ReadTimeoutException &#123;</span><br><span class="line">    List&lt;Row&gt; rows = new ArrayList&lt;&gt;(initialCommands.size());</span><br><span class="line">    // (avoid allocating a new list in the common case of nothing-to-retry)</span><br><span class="line">    List&lt;ReadCommand&gt; commandsToRetry = Collections.emptyList();</span><br><span class="line"></span><br><span class="line">    do &#123;</span><br><span class="line">        List&lt;ReadCommand&gt; commands = commandsToRetry.isEmpty() ? initialCommands : commandsToRetry;</span><br><span class="line">        AbstractReadExecutor[] readExecutors = new AbstractReadExecutor[commands.size()];</span><br><span class="line"></span><br><span class="line">        if (!commandsToRetry.isEmpty())</span><br><span class="line">            Tracing.trace(&quot;Retrying &#123;&#125; commands&quot;, commandsToRetry.size());</span><br><span class="line"></span><br><span class="line">        // 每个ReadCommand由ReadExecutor来执行</span><br><span class="line">        for (int i = 0; i &lt; commands.size(); i++) &#123;</span><br><span class="line">            ReadCommand command = commands.get(i);</span><br><span class="line">            assert !command.isDigestQuery();</span><br><span class="line"></span><br><span class="line">            AbstractReadExecutor exec = AbstractReadExecutor.getReadExecutor(command, consistencyLevel);</span><br><span class="line">            exec.executeAsync();</span><br><span class="line">            readExecutors[i] = exec;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for (AbstractReadExecutor exec : readExecutors)</span><br><span class="line">            exec.maybeTryAdditionalReplicas();</span><br><span class="line"></span><br><span class="line">        // read results and make a second pass for any digest mismatches</span><br><span class="line">        List&lt;ReadCommand&gt; repairCommands = null;</span><br><span class="line">        List&lt;ReadCallback&lt;ReadResponse, Row&gt;&gt; repairResponseHandlers = null;</span><br><span class="line">        for (AbstractReadExecutor exec : readExecutors) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                Row row = exec.get();</span><br><span class="line">                if (row != null) &#123;</span><br><span class="line">                    exec.command.maybeTrim(row);</span><br><span class="line">                    rows.add(row);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if (logger.isDebugEnabled())</span><br><span class="line">                    logger.debug(&quot;Read: &#123;&#125; ms.&quot;, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - exec.handler.start));</span><br><span class="line">            &#125; catch (ReadTimeoutException ex) &#123;</span><br><span class="line">                ......</span><br><span class="line">                throw ex;</span><br><span class="line">            &#125; catch (DigestMismatchException ex) &#123; // 读取修复的处理</span><br><span class="line">                Tracing.trace(&quot;Digest mismatch: &#123;&#125;&quot;, ex);</span><br><span class="line"></span><br><span class="line">                ReadRepairMetrics.repairedBlocking.mark();</span><br><span class="line"></span><br><span class="line">                // 创建一个读取所有数据的ReadCommand</span><br><span class="line">                // Do a full data read to resolve the correct response (and repair node that need be)</span><br><span class="line">                RowDataResolver resolver = new RowDataResolver(exec.command.ksName, exec.command.key,</span><br><span class="line">                        exec.command.filter(), exec.command.timestamp);</span><br><span class="line"></span><br><span class="line">                ReadCallback&lt;ReadResponse, Row&gt; repairHandler = new ReadCallback&lt;&gt;(resolver,</span><br><span class="line">                        ConsistencyLevel.ALL,</span><br><span class="line">                        exec.getContactedReplicas().size(),</span><br><span class="line">                        exec.command,</span><br><span class="line">                        Keyspace.open(exec.command.getKeyspace()),</span><br><span class="line">                        exec.handler.endpoints);</span><br><span class="line"></span><br><span class="line">                if (repairCommands == null) &#123;</span><br><span class="line">                    repairCommands = new ArrayList&lt;&gt;();</span><br><span class="line">                    repairResponseHandlers = new ArrayList&lt;&gt;();</span><br><span class="line">                &#125;</span><br><span class="line">                repairCommands.add(exec.command);</span><br><span class="line">                repairResponseHandlers.add(repairHandler);</span><br><span class="line"></span><br><span class="line">                MessageOut&lt;ReadCommand&gt; message = exec.command.createMessage();</span><br><span class="line">                for (InetAddress endpoint : exec.getContactedReplicas()) &#123;</span><br><span class="line">                    Tracing.trace(&quot;Enqueuing full data read to &#123;&#125;&quot;, endpoint);</span><br><span class="line">                    MessagingService.instance().sendRR(message, endpoint, repairHandler);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        commandsToRetry.clear();</span><br><span class="line"></span><br><span class="line">        // read the results for the digest mismatch retries</span><br><span class="line">        if (repairResponseHandlers != null) &#123;</span><br><span class="line">            for (int i = 0; i &lt; repairCommands.size(); i++) &#123;</span><br><span class="line">                ReadCommand command = repairCommands.get(i);</span><br><span class="line">                ReadCallback&lt;ReadResponse, Row&gt; handler = repairResponseHandlers.get(i);</span><br><span class="line"></span><br><span class="line">                Row row;</span><br><span class="line">                try &#123;</span><br><span class="line">                    row = handler.get();</span><br><span class="line">                &#125; catch (DigestMismatchException e) &#123;</span><br><span class="line">                    throw new AssertionError(e); // full data requested from each node here, no digests should be sent</span><br><span class="line">                &#125; catch (ReadTimeoutException e) &#123;</span><br><span class="line">                    ......</span><br><span class="line">                    throw new ReadTimeoutException(consistencyLevel, blockFor - 1, blockFor, true);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                RowDataResolver resolver = (RowDataResolver) handler.resolver;</span><br><span class="line">                try &#123;</span><br><span class="line">                    // 等待修改写入完成</span><br><span class="line">                    // wait for the repair writes to be acknowledged, to minimize impact on any replica that&#x27;s</span><br><span class="line">                    // behind on writes in case the out-of-sync row is read multiple times in quick succession</span><br><span class="line">                    FBUtilities.waitOnFutures(resolver.repairResults, DatabaseDescriptor.getWriteRpcTimeout());</span><br><span class="line">                &#125; catch (TimeoutException e) &#123;</span><br><span class="line">                    if (Tracing.isTracing())</span><br><span class="line">                        Tracing.trace(&quot;Timed out waiting on digest mismatch repair acknowledgements&quot;);</span><br><span class="line">                    else</span><br><span class="line">                        logger.debug(&quot;Timed out waiting on digest mismatch repair acknowledgements&quot;);</span><br><span class="line">                    int blockFor = consistencyLevel.blockFor(Keyspace.open(command.getKeyspace()));</span><br><span class="line">                    throw new ReadTimeoutException(consistencyLevel, blockFor - 1, blockFor, true);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                // retry any potential short reads 重新读取</span><br><span class="line">                ReadCommand retryCommand = command.maybeGenerateRetryCommand(resolver, row);</span><br><span class="line">                if (retryCommand != null) &#123;</span><br><span class="line">                    Tracing.trace(&quot;Issuing retry for read command&quot;);</span><br><span class="line">                    if (commandsToRetry == Collections.EMPTY_LIST)</span><br><span class="line">                        commandsToRetry = new ArrayList&lt;&gt;();</span><br><span class="line">                    commandsToRetry.add(retryCommand);</span><br><span class="line">                    continue;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                if (row != null) &#123;</span><br><span class="line">                    command.maybeTrim(row);</span><br><span class="line">                    rows.add(row);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; while (!commandsToRetry.isEmpty());</span><br><span class="line"></span><br><span class="line">    return rows;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面针对上面的步骤进行详细说明：</p>
<h3 id="生成并执行ReadExecutor"><a href="#生成并执行ReadExecutor" class="headerlink" title="生成并执行ReadExecutor"></a>生成并执行ReadExecutor</h3><ol>
<li><p>获取所有存活节点，并根据和当前的协调节点的“远近”进行排序，排序的信息由Snitch给出。大部分的Snitch实现继承了AbstractNetworkTopologySnitch抽象类，该抽象类基于网络拓扑信息来排序，具体地讲根据是否同一数据中心，同一机架来判断“远近”。还有一个特殊的Snitch实现DynamicEndpointSnitch，它封装了一个其他的Snitch实现类，在排序时会考虑各个节点的时延，时延数据保存在scores属性中。DynamicEndpointSnitch中有个BADNESS_THRESHOLD参数，当该参数为0，直接使用时延大小来排序；否则的话就进行一个特殊处理：把时延数据拷贝一份，一个根据subsnitch的排序结果来排序（记为subsnitchScore），一个是根据 时延的大小排序（记为sortedScore），如果在相同位置满足subsnitchScore &gt;  (1.0 + BADNESS_THRESHOLD) * sortedScore 就采样时延排序的结果，否则还是采用subsnitchScore的结果。在时延数据采集方面，要考虑采集的频率，以及最近的数据权重越大（使用ExponentiallyDecayingSample），DynamicEndpointSnitch中有个定期执行runnable负责scores数据的更新。此外，DynamicEndpointSnitch还有一个定期任务重置scores，否则故障恢复后的节点会一直被排在很后面而不会被请求数据，因此需要定期重置<sup>1</sup>。</p>
</li>
<li><p>产生随机数，并通过和配置参数的对比，选择ReadRepairDecision。由于hinted handoff的存在，正常情况下读取修复的发生概率不大。</p>
</li>
<li><p>根据consistencyLevel和repairDecision过滤出要访问的节点</p>
</li>
<li><p>根据retryType选择AbstractReadExecutor的子类。如果不考虑retryType，那么在读取时向一个节点读取具体的数据（对应NeverSpeculatingReadExecutor），向其他节点读取数据摘要。如果考虑了retryType（对应SpeculatingReadExecutor或AlwaysSpeculatingReadExecutor），那么可能向多于一个的节点读取具体数据；其中AlwaysSpeculatingReadExecutor总是会向一个附加的节点（如果存在）发送数据请求，SpeculatingReadExecutor会在读取超过sampleLatencyNanos（sampleLatencyNanos &lt; ReadTimeOut）时，对附加节点发起请求，这种方式被称为”rapid read protection”<sup>2</sup>，sampleLatencyNanos可以配置为具体的时间，也可以配置为percentile。</p>
</li>
</ol>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">public static AbstractReadExecutor getReadExecutor(ReadCommand command, ConsistencyLevel consistencyLevel) throws UnavailableException &#123;</span><br><span class="line">       Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">       // 获取所有存活节点，并排序</span><br><span class="line">       List&lt;InetAddress&gt; allReplicas = StorageProxy.getLiveSortedEndpoints(keyspace, command.key);</span><br><span class="line">       // 产生随机数，并通过和配置参数的对比，选择ReadRepairDecision</span><br><span class="line">       ReadRepairDecision repairDecision = Schema.instance.getCFMetaData(command.ksName, command.cfName).newReadRepairDecision();</span><br><span class="line">       // 根据consistencyLevel和repairDecision过滤出要访问的节点</span><br><span class="line">       List&lt;InetAddress&gt; targetReplicas = consistencyLevel.filterForQuery(keyspace, allReplicas, repairDecision);</span><br><span class="line"></span><br><span class="line">       // 判断是否满足一致性要求，不满足的话抛出UnavailableException</span><br><span class="line">       consistencyLevel.assureSufficientLiveNodes(keyspace, targetReplicas);</span><br><span class="line"></span><br><span class="line">       // Fat client. Speculating read executors need access to cfs metrics and sampled latency, and fat clients</span><br><span class="line">       // can&#x27;t provide that. So, for now, fat clients will always use NeverSpeculatingReadExecutor.</span><br><span class="line">       if (StorageService.instance.isClientMode())</span><br><span class="line">           return new NeverSpeculatingReadExecutor(command, consistencyLevel, targetReplicas);</span><br><span class="line"></span><br><span class="line">       if (repairDecision != ReadRepairDecision.NONE)</span><br><span class="line">           ReadRepairMetrics.attempted.mark();</span><br><span class="line"></span><br><span class="line">       ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(command.cfName);</span><br><span class="line">       RetryType retryType = cfs.metadata.getSpeculativeRetry().type;</span><br><span class="line"></span><br><span class="line">       // Speculative retry is disabled *OR* there are simply no extra replicas to speculate.</span><br><span class="line">       if (retryType == RetryType.NONE || consistencyLevel.blockFor(keyspace) == allReplicas.size())</span><br><span class="line">           return new NeverSpeculatingReadExecutor(command, consistencyLevel, targetReplicas);</span><br><span class="line"></span><br><span class="line">       if (targetReplicas.size() == allReplicas.size()) &#123;</span><br><span class="line">           // CL.ALL, RRD.GLOBAL or RRD.DC_LOCAL and a single-DC.</span><br><span class="line">           // We are going to contact every node anyway, so ask for 2 full data requests instead of 1, for redundancy</span><br><span class="line">           // (same amount of requests in total, but we turn 1 digest request into a full blown data request).</span><br><span class="line">           return new AlwaysSpeculatingReadExecutor(cfs, command, consistencyLevel, targetReplicas);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       // RRD.NONE or RRD.DC_LOCAL w/ multiple DCs.</span><br><span class="line">       InetAddress extraReplica = allReplicas.get(targetReplicas.size());</span><br><span class="line">       // With repair decision DC_LOCAL all replicas/target replicas may be in different order, so</span><br><span class="line">       // we might have to find a replacement that&#x27;s not already in targetReplicas.</span><br><span class="line">       if (repairDecision == ReadRepairDecision.DC_LOCAL &amp;&amp; targetReplicas.contains(extraReplica)) &#123;</span><br><span class="line">           for (InetAddress address : allReplicas) &#123;</span><br><span class="line">               if (!targetReplicas.contains(address)) &#123;</span><br><span class="line">                   extraReplica = address;</span><br><span class="line">                   break;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       targetReplicas.add(extraReplica);</span><br><span class="line"></span><br><span class="line">       if (retryType == RetryType.ALWAYS)</span><br><span class="line">           return new AlwaysSpeculatingReadExecutor(cfs, command, consistencyLevel, targetReplicas);</span><br><span class="line">       else // PERCENTILE or CUSTOM.</span><br><span class="line">           return new SpeculatingReadExecutor(cfs, command, consistencyLevel, targetReplicas);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<ol start="5">
<li><p>调用ReadExecutore的executeAsync方法，SpeculatingReadExecutor的executeAsync方法如下，可以看到会向一个副本（最多两个）发送DataRequest，向其他副本发送DigestRequest。如果是请求本节点则向Stage.READ提交一个LocalReadRunnable，里面封装了ReadCommand和ReadCallback回调；如果不是本节点，则向目标节点发送消息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public void executeAsync() &#123;</span><br><span class="line">    // 最后一个副本作为&quot;extraReplica&quot;</span><br><span class="line">    List&lt;InetAddress&gt; initialReplicas = targetReplicas.subList(0, targetReplicas.size() - 1);</span><br><span class="line"></span><br><span class="line">    if (handler.blockfor &lt; initialReplicas.size()) &#123;</span><br><span class="line">        // We&#x27;re hitting additional targets for read repair.  Since our &quot;extra&quot; replica is the least-</span><br><span class="line">        // preferred by the snitch, we do an extra data read to start with against a replica more</span><br><span class="line">        // likely to reply; better to let RR fail than the entire query.</span><br><span class="line">        makeDataRequests(initialReplicas.subList(0, 2)); // TODO ???</span><br><span class="line">        if (initialReplicas.size() &gt; 2)</span><br><span class="line">            makeDigestRequests(initialReplicas.subList(2, initialReplicas.size()));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // not doing read repair; all replies are important, so it doesn&#x27;t matter which nodes we</span><br><span class="line">        // perform data reads against vs digest.</span><br><span class="line">        makeDataRequests(initialReplicas.subList(0, 1));</span><br><span class="line">        if (initialReplicas.size() &gt; 1)</span><br><span class="line">            makeDigestRequests(initialReplicas.subList(1, initialReplicas.size()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>调用各个ReadExecutor的maybeTryAdditionalReplicas()，如果Snitch优先选择的节点没有快速返回，则向extraReplica发起请求。</p>
</li>
<li><p>读取结果会缓存到ReadCallback回调的resolver属性的replies中，当调用Executore的get()方法时，会调用resolver的resolve()方法（正常读取时的resolver是RowDigestResolver对象，如果只读取了一个节点，那么直接调用getData()方法返回读取数据），在reslove()方法中会比较DigestRequest和DataRequest的MD5数据摘要digest，如果有不相符抛出DigestMismatchException，否则返回DataRequest返回的数据内容。</p>
</li>
</ol>
<h3 id="读取修复"><a href="#读取修复" class="headerlink" title="读取修复"></a>读取修复</h3><ol>
<li>当捕捉到ReadTimeoutException异常时，会根据原有的ReadCommand创建一个读取所有可达节点的ReadCommand，同时ReadCallback回调的resolver设置为RowDataResolver实例。</li>
<li>调用RowDataResolver的resolve()方法，在该方法中会对所有节点返回的数据（行数据，即column的迭代器），通过多路合并的方式，组成一条最新的数据（这里可能涉及到版本冲突的处理，Cassandra并没有采用vector clocks来处理，而是把每一列都作为独立操作，每一列都有一个时间戳，根据时间戳来选择数据的版本<sup>3</sup>）。之后调用scheduleRepairs()方法向所有需要更新数据的节点发送写入最新数据的请求，并同步等待这些写入完成。</li>
<li>对于SliceFromReadCommand，可能要生成RetriedSliceFromReadCommand，然后重新执行读取流程。</li>
</ol>
<p>下面再看本地数据读取过程。前面说到ReadExecutor会向Stage.READ中提交一个LocalReadRunnable，该Runnable中主要调用了ColumnFamily的getColumnFamily()方法，该方法的主要逻辑：</p>
<ol>
<li>如果开启了RowCache，则直接从RowCache中读取</li>
<li>如果RowCache中没有相关数据或者没有开启RowCache，则调用getTopLevelColumns()方法。该方法创建了一个CollationController()对象，又调用了它的collectAllData()方法。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public ColumnFamily getColumnFamily(QueryFilter filter) &#123;</span><br><span class="line">    assert name.equals(filter.getColumnFamilyName()) : filter.getColumnFamilyName();</span><br><span class="line"></span><br><span class="line">    ColumnFamily result = null;</span><br><span class="line"></span><br><span class="line">    long start = System.nanoTime();</span><br><span class="line">    try &#123;</span><br><span class="line">        int gcBefore = gcBefore(filter.timestamp);</span><br><span class="line">        if (isRowCacheEnabled()) &#123; // 开启了RowCache</span><br><span class="line">            assert !isIndex(); // CASSANDRA-5732</span><br><span class="line">            UUID cfId = metadata.cfId;</span><br><span class="line"></span><br><span class="line">            ColumnFamily cached = getThroughCache(cfId, filter);</span><br><span class="line">            if (cached == null) &#123;</span><br><span class="line">                logger.trace(&quot;cached row is empty&quot;);</span><br><span class="line">                return null;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            result = filterColumnFamily(cached, filter);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            ColumnFamily cf = getTopLevelColumns(filter, gcBefore);</span><br><span class="line"></span><br><span class="line">            if (cf == null)</span><br><span class="line">                return null;</span><br><span class="line"></span><br><span class="line">            result = removeDeletedCF(cf, gcBefore);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        removeDroppedColumns(result); // 删除已经被drop的列</span><br><span class="line"></span><br><span class="line">        if (filter.filter instanceof SliceQueryFilter) &#123;</span><br><span class="line">           ......  // 更新metric</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        metric.readLatency.addNano(System.nanoTime() - start);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>collectAllData()方法如下，主要是分为两个步骤</p>
<ol>
<li>查找key对应的row，并返回对应的column迭代器</li>
<li>把多个column迭代器进行归并。</li>
</ol>
<p>从代码中可以发现会查询所有的memetable和sstable（根据delete time可以过滤掉一部分无须访问的），而sstable的读取是IO操作，从而可以推断如果sstable数量很大，那么势必会影响到读取性能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">private ColumnFamily collectAllData() &#123;</span><br><span class="line"></span><br><span class="line">    ColumnFamilyStore.ViewFragment view = cfs.markReferenced(filter.key);</span><br><span class="line">    List&lt;OnDiskAtomIterator&gt; iterators = new ArrayList&lt;&gt;(Iterables.size(view.memtables) + view.sstables.size());</span><br><span class="line">    ColumnFamily returnCF = ArrayBackedSortedColumns.factory.create(cfs.metadata, filter.filter.isReversed());</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">        Tracing.trace(&quot;Merging memtable tombstones&quot;);</span><br><span class="line">        for (Memtable memtable : view.memtables) &#123;</span><br><span class="line">            OnDiskAtomIterator iter = filter.getMemtableColumnIterator(memtable);</span><br><span class="line">            if (iter != null) &#123;</span><br><span class="line">                returnCF.delete(iter.getColumnFamily()); // 更新deleteInfo</span><br><span class="line">                iterators.add(iter);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 根据maxTimestamp排序</span><br><span class="line">        Collections.sort(view.sstables, SSTable.maxTimestampComparator);</span><br><span class="line">        List&lt;SSTableReader&gt; skippedSSTables = null;</span><br><span class="line">        long mostRecentRowTombstone = Long.MIN_VALUE;</span><br><span class="line">        long minTimestamp = Long.MAX_VALUE;</span><br><span class="line">        int nonIntersectingSSTables = 0;</span><br><span class="line"></span><br><span class="line">        for (SSTableReader sstable : view.sstables) &#123;</span><br><span class="line">            minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());</span><br><span class="line">           </span><br><span class="line">            // 如果maxTimeStamp都小于row tombstone的时间，那么可以直接跳过</span><br><span class="line">            if (sstable.getMaxTimestamp() &lt; mostRecentRowTombstone)</span><br><span class="line">                break;</span><br><span class="line"></span><br><span class="line">            if (!filter.shouldInclude(sstable)) &#123;</span><br><span class="line">                nonIntersectingSSTables++;</span><br><span class="line">                // sstable contains no tombstone if maxLocalDeletionTime == Integer.MAX_VALUE, so we can safely skip those entirely</span><br><span class="line">                if (sstable.getSSTableMetadata().maxLocalDeletionTime != Integer.MAX_VALUE) &#123;</span><br><span class="line">                    if (skippedSSTables == null)</span><br><span class="line">                        skippedSSTables = new ArrayList&lt;&gt;();</span><br><span class="line">                    skippedSSTables.add(sstable);</span><br><span class="line">                &#125;</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            sstable.incrementReadCount();</span><br><span class="line">            // 根据可以key找到对应的row，返回column的迭代器</span><br><span class="line">            OnDiskAtomIterator iter = filter.getSSTableColumnIterator(sstable);</span><br><span class="line">            iterators.add(iter);</span><br><span class="line">            if (iter.getColumnFamily() != null) &#123;</span><br><span class="line">                ColumnFamily cf = iter.getColumnFamily(); // 返回只包含deleteInfo的cf</span><br><span class="line">                if (cf.isMarkedForDelete())</span><br><span class="line">                    mostRecentRowTombstone = cf.deletionInfo().getTopLevelDeletion().markedForDeleteAt;</span><br><span class="line"></span><br><span class="line">                returnCF.delete(cf); // 更新deleteInfo</span><br><span class="line">                sstablesIterated++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        int includedDueToTombstones = 0;</span><br><span class="line">        // Check for row tombstone in the skipped sstables</span><br><span class="line">        if (skippedSSTables != null) &#123;</span><br><span class="line">            for (SSTableReader sstable : skippedSSTables) &#123;</span><br><span class="line">                if (sstable.getMaxTimestamp() &lt;= minTimestamp)</span><br><span class="line">                    continue;</span><br><span class="line"></span><br><span class="line">                sstable.incrementReadCount();</span><br><span class="line">                OnDiskAtomIterator iter = filter.getSSTableColumnIterator(sstable);</span><br><span class="line">                ColumnFamily cf = iter.getColumnFamily();</span><br><span class="line">                // we are only interested in row-level tombstones here, and only if markedForDeleteAt is larger than minTimestamp</span><br><span class="line">                if (cf != null &amp;&amp; cf.deletionInfo().getTopLevelDeletion().markedForDeleteAt &gt; minTimestamp) &#123;</span><br><span class="line">                    includedDueToTombstones++;</span><br><span class="line">                    iterators.add(iter);</span><br><span class="line">                    returnCF.delete(cf.deletionInfo().getTopLevelDeletion());</span><br><span class="line">                    sstablesIterated++;</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    FileUtils.closeQuietly(iter);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        // we need to distinguish between &quot;there is no data at all for this row&quot; (BF will let us rebuild that efficiently)</span><br><span class="line">        // and &quot;there used to be data, but it&#x27;s gone now&quot; (we should cache the empty CF so we don&#x27;t need to rebuild that slower)</span><br><span class="line">        if (iterators.isEmpty())</span><br><span class="line">            return null;</span><br><span class="line"></span><br><span class="line">        Tracing.trace(&quot;Merging data from memtables and &#123;&#125; sstables&quot;, sstablesIterated);</span><br><span class="line">        filter.collateOnDiskAtom(returnCF, iterators, gcBefore); // 合并数据</span><br><span class="line"></span><br><span class="line">        // Caller is responsible for final removeDeletedCF.  This is important for cacheRow to work correctly:</span><br><span class="line">        return returnCF;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        for (OnDiskAtomIterator iter : iterators)</span><br><span class="line">            FileUtils.closeQuietly(iter);</span><br><span class="line">        SSTableReader.releaseReferences(view.sstables);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="获取column迭代器"><a href="#获取column迭代器" class="headerlink" title="获取column迭代器"></a>获取column迭代器</h3><p>获取column迭代器就是读取sstable文件的过程。调用<code>filter.getSSTableColumnIterator(sstable)</code>生成了一个SSTableSliceIterator对象，看下SSTableSliceIterator的构造方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public SSTableSliceIterator(SSTableReader sstable, DecoratedKey key, ColumnSlice[] slices, boolean reversed) &#123;</span><br><span class="line">    this.key = key;</span><br><span class="line">    // 查找RowIndexEntry</span><br><span class="line">    RowIndexEntry indexEntry = sstable.getPosition(key, SSTableReader.Operator.EQ);</span><br><span class="line">    // 创建OnDiskAtomIterator</span><br><span class="line">    this.reader = indexEntry == null ? null : createReader(sstable, indexEntry, null, slices, reversed);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中getPosition方法主要有一下几步：</p>
<ol>
<li><p>如果操作符是等于(EQ)，则先在BloomFilter中查询是否可能存在该key</p>
</li>
<li><p>如果操作符是等于(EQ)或大于等于(GE)，则在RowCache（默认开启）中查找是否缓存过对应的RowIndexEntry，如果是则返回，否则进入下一步。</p>
</li>
<li><p>在IndexSummary查找大于等于key的第一项，那么从该项的前一项所指向的位置（IndexSummary能保存的是key在索引文件中的位置）开始在索引文件中查找，并返回满足条件的索引项。如果要查找的key小于IndexSummary的第一项（也就是索引文件中的最小key），那么说明该sstable中不存在该key（对于&lt;, = 或 &gt;= 操作），可以跳过。</p>
</li>
</ol>
<p>获取到索引项之后就可以知道要读取的row在数据文件中的位置，然后调用createReader()方法创建一个OnDiskAtomIterator，以SimpleSliceReader类为例，该类的构造方法中首先把数据文件定位到索引项所指的位置，然后读取row的deleteInfo并保存一个空的ColumnFamily对象中，最后创建一个column的迭代器。</p>
<h3 id="column迭代器的多路归并"><a href="#column迭代器的多路归并" class="headerlink" title="column迭代器的多路归并"></a>column迭代器的多路归并</h3><p>在前一步已经返回了多个column的迭代器，下面对这些迭代器进行多路归并，归并的目的是把各个sstable中不同版本的用一列数据进行统一。主要逻辑如下，其中多路合并主要利用了MergeIterator类，此外合并前的iterator中的元素必须是有序。MergeIterator的多路归并实现中，主要利用了最小堆，把各个iterator根据当前的元素放入堆中，然后查看堆顶的iterator，如果堆顶iterator的当前元素值和前一个不同，说明是一个新的值了，那么就对之前取出的元素应用reducer。堆顶取出的itrator如果还有元素需要重新放入队中。如此循环直到所有的iterator都遍历完成。MergeIterator是一个优雅的实现，返回的也是一个iterator，并不是立即计算的，也就是说是一种lazily evaluation。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">public static void collateOnDiskAtom(final ColumnFamily returnCF, List&lt;? extends Iterator&lt;? extends OnDiskAtom&gt;&gt; toCollate,</span><br><span class="line">                                     IDiskAtomFilter filter, int gcBefore, long timestamp) &#123;</span><br><span class="line">    List&lt;Iterator&lt;Column&gt;&gt; filteredIterators = new ArrayList&lt;&gt;(toCollate.size());</span><br><span class="line">    for (Iterator&lt;? extends OnDiskAtom&gt; iter : toCollate)</span><br><span class="line">        // 迭代iterator的时候跳过tombstone，tombstone存到returnCF</span><br><span class="line">        filteredIterators.add(gatherTombstones(returnCF, iter));</span><br><span class="line">    collateColumns(returnCF, filteredIterators, filter, gcBefore, timestamp);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public static void collateColumns(final ColumnFamily returnCF, List&lt;? extends Iterator&lt;Column&gt;&gt; toCollate,</span><br><span class="line">                                  IDiskAtomFilter filter, int gcBefore, long timestamp) &#123;</span><br><span class="line">    final Comparator&lt;Column&gt; fcomp = filter.getColumnComparator(returnCF.getComparator());</span><br><span class="line">    // 定义reducer，处理具有相同名称的column</span><br><span class="line">    MergeIterator.Reducer&lt;Column, Column&gt; reducer = new MergeIterator.Reducer&lt;Column, Column&gt;() &#123;</span><br><span class="line">        Column current;</span><br><span class="line"></span><br><span class="line">        public void reduce(Column next) &#123;</span><br><span class="line">            assert current == null || fcomp.compare(current, next) == 0;</span><br><span class="line">            // reconcile根据时间戳获取最新版本的数据</span><br><span class="line">            current = current == null ? next : current.reconcile(next, HeapAllocator.instance);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        protected Column getReduced() &#123;</span><br><span class="line">            assert current != null;</span><br><span class="line">            Column toReturn = current;</span><br><span class="line">            current = null;</span><br><span class="line">            return toReturn;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    Iterator&lt;Column&gt; reduced = MergeIterator.get(toCollate, fcomp, reducer);</span><br><span class="line"></span><br><span class="line">    filter.collectReducedColumns(returnCF, reduced, gcBefore, timestamp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>多路归并之后，再调用collectReducedColumns进行处理。首先构建一个ColumnCounter，需要注意的是，这里的ColumnCounter并不是列数量计数，而是列分组的计数，分组的依据是列名有相同的前缀，而前缀就是clustering key。回忆下，在写入的时候，列名被改成了clustering key + column name。 因此同一个分组的各个column组成了我们通常意义上的一行记录。然后就遍历归并后的column iterator，如果该column没有被删除或过期，则加入到columnfamily中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public void collectReducedColumns(ColumnFamily container, Iterator&lt;Column&gt; reducedColumns, int gcBefore, long now) &#123;</span><br><span class="line">    // 此处是分组基数</span><br><span class="line">    columnCounter = columnCounter(container.getComparator(), now);</span><br><span class="line">    // 多路归并的时候把所有的tomestone都合并了</span><br><span class="line">    DeletionInfo.InOrderTester tester = container.deletionInfo().inOrderTester(reversed);</span><br><span class="line"></span><br><span class="line">    while (reducedColumns.hasNext()) &#123;</span><br><span class="line">        Column column = reducedColumns.next();</span><br><span class="line"></span><br><span class="line">        columnCounter.count(column, tester);</span><br><span class="line"></span><br><span class="line">        if (columnCounter.live() &gt; count) // 大于limit设置的数量，不再读取</span><br><span class="line">            break;</span><br><span class="line"></span><br><span class="line">        // tombstone过多，直接报错        </span><br><span class="line">        if (respectTombstoneThresholds() &amp;&amp; columnCounter.ignored() &gt; DatabaseDescriptor.getTombstoneFailureThreshold()) &#123;</span><br><span class="line">            ......</span><br><span class="line">            throw new TombstoneOverwhelmingException();</span><br><span class="line">        &#125;</span><br><span class="line">        // 没有被删除的column放入到columnfamily中</span><br><span class="line">        container.addIfRelevant(column, tester, gcBefore);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    if (respectTombstoneThresholds() &amp;&amp; columnCounter.ignored() &gt; DatabaseDescriptor.getTombstoneWarnThreshold()) &#123;</span><br><span class="line">         // 打印warn日志</span><br><span class="line">         ......</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此fetchRows()的流程结束，再次回到SelectStatement的execute()方法中，再返回最终结果前，还要调用processResults()进行处理，该方法的主要逻辑还是先把row中的column按照clustering key分组，然后根据把分组创建成寻常意义上的记录行，并添加上需要的partition key，最后封装成ResultSet返回。</p>
<p>Reference</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://www.datastax.com/dev/blog/dynamic-snitching-in-cassandra-past-present-and-future">https://www.datastax.com/dev/blog/dynamic-snitching-in-cassandra-past-present-and-future</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.datastax.com/dev/blog/rapid-read-protection-in-cassandra-2-0-2">https://www.datastax.com/dev/blog/rapid-read-protection-in-cassandra-2-0-2</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks">https://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks</a></p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/09/23/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%945-SELECT%E6%93%8D%E4%BD%9C/" data-id="ckzldzrau000xtofw539n91cg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Cassandra源码阅读随笔4-哈希环和hinted-handoff" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/20/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%944-%E5%93%88%E5%B8%8C%E7%8E%AF%E5%92%8Chinted-handoff/" class="article-date">
  <time datetime="2019-09-20T09:14:42.000Z" itemprop="datePublished">2019-09-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/20/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%944-%E5%93%88%E5%B8%8C%E7%8E%AF%E5%92%8Chinted-handoff/">Cassandra源码阅读随笔4. 哈希环和hinted handoff</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="哈希环和token"><a href="#哈希环和token" class="headerlink" title="哈希环和token"></a>哈希环和token</h2><p>在INSERT插入流程中提到，首先会根据数据的partition key确定要插入的节点。我们知道Cassandra使用一致性哈希把所有节点映射到一个哈希环上，然后根据partition key的哈希值来确定具体到哪个节点上。为了使得数据分布均匀，引入了虚拟节点（VirtualNode/VNode）的概念，即一个物理节点对应哈希环上的多个点。Cassandra在启动的时候，如果本节点尚未分配有token（一个token表示一个虚拟节点），会掉哦给你BootStrapper的getRandomTokens()方法生成num_token个随机token，num_token默认是256，即默认一个物理节点会对应256个虚拟节点，可以看到这个数量还是挺大的，因为是采用的随机生成方法，只要生成的数量足够多，才可能分配均匀。</p>
<p>集群中的节点会交换各自生成的token信息，这样就可以确定每个节点在哈希环上覆盖的范围。举个例子，假如现在有2个物理节点，每个节点对应3个虚拟节点，token范围为0-100，节点1生成的token分别为10,45,70，节点2生成的token分别为30,65,90，那么节点2负责的范围(10, 30],(45,65]和(70, 90]，其他范围由节点1负责。</p>
<p>除了随机生成token，还可以使用官方提供的tokengentool来手动生成token。</p>
<p>回头看插入过程中，获取目标节点的调用如下，StorageService的getNaturalEndpoints会调用AbstractReplicationStrategy类的同名方法，可以返回的并不只有一个节点，而是一个列表，因为根据复制策略和复制因子（replication factor），可以写入多个副本，比如复制因子为3，那么就要写入到3个节点中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// StorageProxy.performWrite()方法中</span><br><span class="line">List&lt;InetAddress&gt; naturalEndpoints = StorageService的.instance.getNaturalEndpoints(keyspaceName, tk);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// AbstractReplicationStrategy类</span><br><span class="line">public ArrayList&lt;InetAddress&gt; getNaturalEndpoints(RingPosition searchPosition) &#123;</span><br><span class="line">       Token searchToken = searchPosition.getToken();</span><br><span class="line">       // 找到 &gt;= searchToken的第一个token</span><br><span class="line">       Token keyToken = TokenMetadata.firstToken(tokenMetadata.sortedTokens(), searchToken);</span><br><span class="line">       // 是否有缓存的结果</span><br><span class="line">       ArrayList&lt;InetAddress&gt; endpoints = getCachedEndpoints(keyToken);</span><br><span class="line">       if (endpoints == null) &#123;</span><br><span class="line">           TokenMetadata tm = tokenMetadata.cachedOnlyTokenMap();</span><br><span class="line">           // 新的token加入会使得缓存失效</span><br><span class="line">           // if our cache got invalidated, it&#x27;s possible there is a new token to account for too</span><br><span class="line">           keyToken = TokenMetadata.firstToken(tm.sortedTokens(), searchToken);</span><br><span class="line">           // 根据复制策略和复制因子获取所有的目标节点</span><br><span class="line">           endpoints = new ArrayList&lt;&gt;(calculateNaturalEndpoints(searchToken, tm));</span><br><span class="line">           cachedEndpoints.put(keyToken, endpoints);</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       return new ArrayList&lt;&gt;(endpoints);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>除了根据token找到的节点，看下其他副本的节点是如何查找的，calculateNaturalEndpoints是一个抽象方法，由AbstractReplicationStrategy的子类实现。看下默认使用的复制策略SimpleStrategy的calculateNaturalEndpoints()方法，实现比较简单，就是根据复制因子在哈希环上找到大于等于要查找token的多个token，然后获取其对应的节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;InetAddress&gt; calculateNaturalEndpoints(Token token, TokenMetadata metadata) &#123;</span><br><span class="line">       int replicas = getReplicationFactor();</span><br><span class="line">       ArrayList&lt;Token&gt; tokens = metadata.sortedTokens();</span><br><span class="line">       List&lt;InetAddress&gt; endpoints = new ArrayList&lt;&gt;(replicas);</span><br><span class="line"></span><br><span class="line">       if (tokens.isEmpty())</span><br><span class="line">           return endpoints;</span><br><span class="line"></span><br><span class="line">       // Add the token at the index by default</span><br><span class="line">       Iterator&lt;Token&gt; iter = TokenMetadata.ringIterator(tokens, token, false);</span><br><span class="line">       while (endpoints.size() &lt; replicas &amp;&amp; iter.hasNext()) &#123;</span><br><span class="line">           InetAddress ep = metadata.getEndpoint(iter.next());</span><br><span class="line">           if (!endpoints.contains(ep))</span><br><span class="line">               endpoints.add(ep);</span><br><span class="line">       &#125;</span><br><span class="line">       return endpoints;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>在生成环境中，推荐使用另外一种复制策略NetworkTopologyStrategy，该策略可以配置在每个数据中心的副本数。其calculateNaturalEndpoints()方法的主要逻辑如下，可以看到在同一个数据中心，首先选择不同的机架上的节点，如果所有机架都遍历完还是不够副本数，再从已经遍历的机架上再次选择其他节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;InetAddress&gt; calculateNaturalEndpoints(Token searchToken, TokenMetadata tokenMetadata) &#123;</span><br><span class="line">       // we want to preserve insertion order so that the first added endpoint becomes primary</span><br><span class="line">       Set&lt;InetAddress&gt; replicas = new LinkedHashSet&lt;&gt;();</span><br><span class="line">       // replicas we have found in each DC</span><br><span class="line">       Map&lt;String, Set&lt;InetAddress&gt;&gt; dcReplicas = new HashMap&lt;String, Set&lt;InetAddress&gt;&gt;(datacenters.size()) &#123;&#123;</span><br><span class="line">           for (Map.Entry&lt;String, Integer&gt; dc : datacenters.entrySet())</span><br><span class="line">               put(dc.getKey(), new HashSet&lt;&gt;(dc.getValue()));</span><br><span class="line">       &#125;&#125;;</span><br><span class="line">       </span><br><span class="line">       ......</span><br><span class="line"></span><br><span class="line">       Iterator&lt;Token&gt; tokenIter = TokenMetadata.ringIterator(tokenMetadata.sortedTokens(), searchToken, false);</span><br><span class="line">       while (tokenIter.hasNext() &amp;&amp; !hasSufficientReplicas(dcReplicas, allEndpoints)) &#123;</span><br><span class="line">           Token next = tokenIter.next();</span><br><span class="line">           InetAddress ep = tokenMetadata.getEndpoint(next);</span><br><span class="line">           String dc = snitch.getDatacenter(ep);</span><br><span class="line">           // have we already found all replicas for this dc? 是否有足够的数量</span><br><span class="line">           if (!datacenters.containsKey(dc) || hasSufficientReplicas(dc, dcReplicas, allEndpoints))</span><br><span class="line">               continue;</span><br><span class="line">           // can we skip checking the rack?</span><br><span class="line">           if (seenRacks.get(dc).size() == racks.get(dc).keySet().size()) &#123;</span><br><span class="line">               dcReplicas.get(dc).add(ep);</span><br><span class="line">               replicas.add(ep);</span><br><span class="line">           &#125; else &#123;</span><br><span class="line">               String rack = snitch.getRack(ep);</span><br><span class="line">               // is this a new rack?</span><br><span class="line">               if (seenRacks.get(dc).contains(rack)) &#123; // 该机架已经分配过，先跳过</span><br><span class="line">                   skippedDcEndpoints.get(dc).add(ep);</span><br><span class="line">               &#125; else &#123;</span><br><span class="line">                   dcReplicas.get(dc).add(ep);</span><br><span class="line">                   replicas.add(ep);</span><br><span class="line">                   seenRacks.get(dc).add(rack);</span><br><span class="line">                   // if we&#x27;ve run out of distinct racks, add the hosts we skipped past already (up to RF)</span><br><span class="line">                   if (seenRacks.get(dc).size() == racks.get(dc).keySet().size()) &#123; // 所有机架都已经看过</span><br><span class="line">                       Iterator&lt;InetAddress&gt; skippedIt = skippedDcEndpoints.get(dc).iterator();</span><br><span class="line">                       // 如果还不够副本数，则再从已经分配过的机架上再次分配</span><br><span class="line">                       while (skippedIt.hasNext() &amp;&amp; !hasSufficientReplicas(dc, dcReplicas, allEndpoints)) &#123;</span><br><span class="line">                           InetAddress nextSkipped = skippedIt.next();</span><br><span class="line">                           dcReplicas.get(dc).add(nextSkipped);</span><br><span class="line">                           replicas.add(nextSkipped); // replicas是set，因此跳过是不是同一个节点的判断</span><br><span class="line">                       &#125;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       return new ArrayList&lt;&gt;(replicas);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<h2 id="hintted-handoff"><a href="#hintted-handoff" class="headerlink" title="hintted handoff"></a>hintted handoff</h2><p>在实际环境中，可能由于网络分区、硬件故障等导致写入所属的节点无法访问。为了确保可用性，Cassandra引入了一种称为hinted handoff特性，即当要写入的节点A发生故障时，协调者会创建一个保存写入相关信息的hint，并把该hint保存到其他可用的节点B上，当节点A重新上线后，节点B会把hint发送给节点A。Cassandra为每个分区都提供了可供写入的hint<sup>1</sup>。</p>
<p>在写入过程的sendToHintedEndpoints()方法中，如果某个目标节点被诊断为故障，那么可能会使用hintted handoff。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if (!shouldHint(destination))</span><br><span class="line">		continue;</span><br><span class="line"></span><br><span class="line">   // Schedule a local hint</span><br><span class="line">   submitHint(rm, destination, responseHandler);</span><br></pre></td></tr></table></figure>
<p>在shouldHint中，一个比较重要的判断是<code>boolean hintWindowExpired = Gossiper.instance.getEndpointDowntime(ep) &gt; DatabaseDescriptor.getMaxHintWindow();</code>即机器故障时间要小于配置的max_hint_window_in_ms，否则不会使用hint。</p>
<p>submitHint向HintedHandOffManager提交一个Runaable，其中的writeHintForMutation()方法又调用了HintedHandOffManager的hintFor()方法，该方法中把该要更新的记录序列化为二进制数据，然后作为值构成一条系统空间中hints表的记录[TODO没有看到该记录的写入调用???]。该记录需要设置TTL，其值为mutation中各个columnfamily的gcGraceSeconds的最小值，这是为了防止对记录的删除被之后恢复hint时覆盖。比如一个节点短暂下线了，下线期间保存了一条对记录A的hint在其他节点，该节点下线后马上恢复，之后有个删除记录A的请求，并且在gcGraceSeconds之后，删除记录A的tombstone被清理，如果恢复记录A的hint，那么记录A就“死而复生”了。如果把TTL设置为小于gcGraceSeconds就不会有这个问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">public static Future&lt;Void&gt; submitHint(final RowMutation mutation,</span><br><span class="line">                                      final InetAddress target,</span><br><span class="line">                                      final AbstractWriteResponseHandler responseHandler) &#123;</span><br><span class="line">    // local write that time out should be handled by LocalMutationRunnable</span><br><span class="line">    assert !target.equals(FBUtilities.getBroadcastAddress()) : target;</span><br><span class="line"></span><br><span class="line">    HintRunnable runnable = new HintRunnable(target) &#123;</span><br><span class="line">        public void runMayThrow() &#123;</span><br><span class="line">            int ttl = HintedHandOffManager.calculateHintTTL(mutation);</span><br><span class="line">            if (ttl &gt; 0) &#123;</span><br><span class="line">                logger.debug(&quot;Adding hint for &#123;&#125;&quot;, target);</span><br><span class="line">                writeHintForMutation(mutation, System.currentTimeMillis(), ttl, target);</span><br><span class="line">                // Notify the handler only for CL == ANY</span><br><span class="line">                if (responseHandler != null &amp;&amp; responseHandler.consistencyLevel == ConsistencyLevel.ANY)</span><br><span class="line">                    responseHandler.response(null);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                logger.debug(&quot;Skipped writing hint for &#123;&#125; (ttl &#123;&#125;)&quot;, target, ttl);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    return submitHint(runnable);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// HintedHandOffManager</span><br><span class="line">public RowMutation hintFor(RowMutation mutation, long now, int ttl, UUID targetId) &#123;</span><br><span class="line">    assert ttl &gt; 0;</span><br><span class="line"></span><br><span class="line">    InetAddress endpoint = StorageService.instance.getTokenMetadata().getEndpointForHostId(targetId);</span><br><span class="line">    // during tests we may not have a matching endpoint, but this would be unexpected in real clusters</span><br><span class="line">    if (endpoint != null)</span><br><span class="line">        metrics.incrCreatedHints(endpoint);</span><br><span class="line">    else</span><br><span class="line">        logger.warn(&quot;Unable to find matching endpoint for target &#123;&#125; when storing a hint&quot;, targetId);</span><br><span class="line"></span><br><span class="line">    UUID hintId = UUIDGen.getTimeUUID();</span><br><span class="line">    // serialize the hint with id and version as a composite column name</span><br><span class="line">    ByteBuffer name = comparator.decompose(hintId, MessagingService.current_version);</span><br><span class="line">    ByteBuffer value = ByteBuffer.wrap(FBUtilities.serialize(mutation, RowMutation.serializer, MessagingService.current_version));</span><br><span class="line">    ColumnFamily cf = ArrayBackedSortedColumns.factory.create(Schema.instance.getCFMetaData(Keyspace.SYSTEM_KS, SystemKeyspace.HINTS_CF));</span><br><span class="line">    cf.addColumn(name, value, now, ttl);</span><br><span class="line">    return new RowMutation(Keyspace.SYSTEM_KS, UUIDType.instance.decompose(targetId), cf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>系统启动的时候会调用HintedHandOffManager的start()方法来启动一个每10分钟执行的任务scheduleAllDeliveries，实际的hint读取和发送在doDeliverHintsToEndpoint()方法中，在发送的时候为了避免占用太多带宽，需要进行限流。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">private void scheduleAllDeliveries() &#123;</span><br><span class="line">    // 在发送hints之前，对hints表执行合并操作来避免发送tombstone和过期的hints。</span><br><span class="line">    compact();</span><br><span class="line"></span><br><span class="line">    IPartitioner p = StorageService.getPartitioner();</span><br><span class="line">    RowPosition minPos = p.getMinimumToken().minKeyBound();</span><br><span class="line">    Range&lt;RowPosition&gt; range = new Range&lt;&gt;(minPos, minPos, p);</span><br><span class="line">    IDiskAtomFilter filter = new NamesQueryFilter(ImmutableSortedSet.of());</span><br><span class="line">    // 获取所有数据</span><br><span class="line">    List&lt;Row&gt; rows = hintStore.getRangeSlice(range, null, filter, Integer.MAX_VALUE, System.currentTimeMillis());</span><br><span class="line">    for (Row row : rows) &#123;</span><br><span class="line">        UUID hostId = UUIDGen.getUUID(row.key.key);</span><br><span class="line">        InetAddress target = StorageService.instance.getTokenMetadata().getEndpointForHostId(hostId);</span><br><span class="line">        // token may have since been removed (in which case we have just read back a tombstone)</span><br><span class="line">        if (target != null)</span><br><span class="line">            scheduleHintDelivery(target, false);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    logger.debug(&quot;Finished scheduleAllDeliveries&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在前文提到，hinted handoff可以确保可用性，实际上这个和一致性级别（ConsistencyLevel）有关。从sendToHintedEndpoints()的方法注释可以发现，只有一致性级别设置为非ANY时，hinted handoff并不会参与到一致性级别的计数中，如果没有足够的节点满足写的一致性级别需要，会抛出UnavailableException<sup>1</sup>。</p>
<table>
<thead>
<tr>
<th>Hinted Handoff</th>
<th>Consist.Level</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>on</td>
<td>&gt;=1</td>
<td>wait for hints. We DO NOT notify the handler with handler.response() for hints</td>
</tr>
<tr>
<td>on</td>
<td>ANY</td>
<td>wait for hints. Responses count towards consistency</td>
</tr>
<tr>
<td>off</td>
<td>&gt;=1</td>
<td>DO NOT fire hints. And DO NOT wait for them to complete</td>
</tr>
</tbody></table>
<p>reference </p>
<ol>
<li>Cassandra权威指南</li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sing1ee/archive/2012/07/09/2765056.html">https://www.cnblogs.com/sing1ee/archive/2012/07/09/2765056.html</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/09/20/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%944-%E5%93%88%E5%B8%8C%E7%8E%AF%E5%92%8Chinted-handoff/" data-id="ckzldzrac0003tofw30us7ry0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Cassandra源码阅读随笔3-SSTable的写入" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/15/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%943-SSTable%E7%9A%84%E5%86%99%E5%85%A5/" class="article-date">
  <time datetime="2019-09-15T01:24:11.000Z" itemprop="datePublished">2019-09-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/15/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%943-SSTable%E7%9A%84%E5%86%99%E5%85%A5/">Cassandra源码阅读随笔3. SSTable的写入</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>从前面知道，我们知道更新操作都是在memtable中完成，在一定条件下需要memtable刷到磁盘。一是为了防止memtable占用内存过大，二是为了回收commit log文件。那这个刷入磁盘条件如何判断？在cassandra的发展过程中，出现过几种策略：</p>
<ol>
<li>定期刷入磁盘（配置memtable_flush_after参数）</li>
<li>根据更新次数（配置memtable_operations参数）</li>
<li>根据吞吐量（配置memtable_throughput参数）</li>
<li>根据内存占用情况（配置memtable_total_space_in_mb参数，默认值是堆内存的1/4）</li>
</ol>
<p>显然第4种策略是最合理的。 那么如何测量内存占用呢？Cassandra中使用jamm这个包，但实际上并不是每次都去准确测量内存占用的（因为很慢，特别是memtable很大的时候）。Cassandra中有个liveRatio属性，该属性的含义是内存占用大小和序列化大小的比例，我们知道内存的java对象除了实际的数据外，还包括对象头等信息，那么一个对象的内存占用是比序列化大小更大的。liveRatio的范围是[1, 64]，默认值是10。在Memtable中，有个currentSize属性，该属性记录了memtable序列化之后的大小，每次写入操作都会更新该值。有个了currentSize之后，再乘以liveRatio就可以估算出当前的内存占用了。现在的问题又变成了如何计算和更新liveRatio。</p>
<p>Memtable的类属性中有个meterExecutor，这是一个单线程的ExecutorService，负责执行测量memtable内存占用的runnable。 Memetable在完成一次写入后，会调用maybeUpdateLiveRatio()方法来判断是否要进行内存占用的测量，判断的依据是当前对memtable的操作次数是上一次测量时的操作次数的10倍（此处的操作次数不是指一次数据写入，而是指一次写入中的列数，再加上deleteInfo相关的数据数量）。从这个依据来看，随着memtable的写入，内存测量的频率会也越来越低，这是合理的，因为总体上数据越来越多，多次算出的liveRatio会越来越接近，从而可以减少计算次数。当需要测量内存的时候会向meterExecutor提交一个MeteringRunnable，在计算出新的ratio，如何比原来的小，那么会保守地设置为<code> memtable.liveRatio = (memtable.liveRatio + newRatio) / 2.0</code>，这样可以尽量减小把内存占用估低从而带来OOM的风险。</p>
<p>Cassandra在启动的时候会启动一个MeteredFlusher单例，之后每隔1秒meteredFlusher会执行一次，看下其run()方法，从中可见不止在所有memtable所占内存超过大小时会flush，如果某个columnfFamilyStore的memtable超过一定的大小，也会开始启动flush。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">public void run() &#123;</span><br><span class="line"></span><br><span class="line">       long allowedSize = calculateAllowedSize();</span><br><span class="line"></span><br><span class="line">       // find how much memory non-active memtables are using</span><br><span class="line">       // 计算非活跃（等待flush）的memtable占用的大小</span><br><span class="line">       long flushingSize = calculateFlushingSize();</span><br><span class="line">       if (flushingSize &gt; 0)</span><br><span class="line">           logger.debug(&quot;Currently flushing &#123;&#125; bytes of &#123;&#125; max&quot;, flushingSize, allowedSize);</span><br><span class="line"></span><br><span class="line">       List&lt;ColumnFamilyStore&gt; affectedCFs = affectedColumnFamilies();</span><br><span class="line">       long liveSize = 0;</span><br><span class="line"></span><br><span class="line">       // flush CFs using more than 1 / (maximum number of memtables it could have in the pipeline)</span><br><span class="line">       // of the total size allotted. Then, flush other CFs in order of size if necessary.</span><br><span class="line">       for (ColumnFamilyStore cfs : affectedCFs) &#123;</span><br><span class="line">           int maxInFlight = (int) Math.ceil((double) (1 // live memtable</span><br><span class="line">                   + 1 // potentially a flushed memtable being counted by jamm</span><br><span class="line">                   + DatabaseDescriptor.getFlushWriters()</span><br><span class="line">                   + DatabaseDescriptor.getFlushQueueSize())</span><br><span class="line">                   / (1 + cfs.indexManager.getIndexesBackedByCfs().size())); // TODO</span><br><span class="line">           long size = cfs.getTotalMemtableLiveSize();</span><br><span class="line">           // 大小超过允许值</span><br><span class="line">           if (allowedSize &gt; flushingSize &amp;&amp; size &gt; (allowedSize - flushingSize) / maxInFlight) &#123;</span><br><span class="line">               logger.info(&quot;Flushing high-traffic column family &#123;&#125; (estimated &#123;&#125; bytes)&quot;, cfs, size);</span><br><span class="line">               cfs.forceFlush();</span><br><span class="line">           &#125; else &#123;</span><br><span class="line">               liveSize += size;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       if (liveSize + flushingSize &lt;= allowedSize)</span><br><span class="line">           return;</span><br><span class="line">       logger.info(&quot;Estimated &#123;&#125; live and &#123;&#125; flushing bytes used by all memtables&quot;, liveSize, flushingSize);</span><br><span class="line"></span><br><span class="line">       Collections.sort(affectedCFs, new Comparator&lt;ColumnFamilyStore&gt;() &#123;</span><br><span class="line">           public int compare(ColumnFamilyStore lhs, ColumnFamilyStore rhs) &#123;</span><br><span class="line">               return Long.compare(lhs.getTotalMemtableLiveSize(), rhs.getTotalMemtableLiveSize());</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;);</span><br><span class="line"></span><br><span class="line">       // 如果已经超过大小限制，按照memtable占用大小排序，从大到小flush，直到占用内存回到设定阈值以下</span><br><span class="line">       // flush largest first until we get below our threshold.</span><br><span class="line">       // although it looks like liveSize + flushingSize will stay a constant, it will not if flushes finish</span><br><span class="line">       // while we loop, which is especially likely to happen if the flush queue fills up (so further forceFlush calls block)</span><br><span class="line">       while (!affectedCFs.isEmpty()) &#123;</span><br><span class="line">           flushingSize = calculateFlushingSize();</span><br><span class="line">           if (liveSize + flushingSize &lt;= allowedSize)</span><br><span class="line">               break;</span><br><span class="line"></span><br><span class="line">           ColumnFamilyStore cfs = affectedCFs.remove(affectedCFs.size() - 1);</span><br><span class="line">           long size = cfs.getTotalMemtableLiveSize();</span><br><span class="line">           if (size &gt; 0) &#123;</span><br><span class="line">               logger.info(&quot;Flushing &#123;&#125; to free up &#123;&#125; bytes&quot;, cfs, size);</span><br><span class="line">               liveSize -= size;</span><br><span class="line">               cfs.forceFlush();</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       logger.trace(&quot;Memtable memory usage is &#123;&#125; bytes with &#123;&#125; live&quot;, liveSize + flushingSize, liveSize);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>columnFamilyStore的forceFlush()方法主要调用了该类中的switchMemtable()方法，该方法中主要是向flushWriter提交一个FlushRunnable以及向postFlushExecutor中提交flush完成需要进行的操作。因为在该方法中要获取全局的switchLock，而flush操作是I/O操作，肯定是要异步执行的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">public Future&lt;?&gt; switchMemtable(final boolean writeCommitLog, boolean forceSwitch) &#123;</span><br><span class="line">       /*</span><br><span class="line">        * If we can get the writelock, that means no new updates can come in and</span><br><span class="line">        * all ongoing updates to memtables have completed. We can get the tail</span><br><span class="line">        * of the log and use it as the starting position for log replay on recovery.</span><br><span class="line">        *</span><br><span class="line">        * This is why we Keyspace.switchLock needs to be global instead of per-Keyspace:</span><br><span class="line">        * we need to schedule discardCompletedSegments calls in the same order as their</span><br><span class="line">        * contexts (commitlog position) were read, even though the flush executor</span><br><span class="line">        * is multithreaded.</span><br><span class="line">        */</span><br><span class="line">       Keyspace.switchLock.writeLock().lock();</span><br><span class="line">       try &#123;</span><br><span class="line">           final Future&lt;ReplayPosition&gt; ctx = writeCommitLog ? CommitLog.instance.getContext() :</span><br><span class="line">                   Futures.immediateFuture(ReplayPosition.NONE);</span><br><span class="line"></span><br><span class="line">           // submit the memtable for any indexed sub-cfses, and our own.</span><br><span class="line">           final List&lt;ColumnFamilyStore&gt; icc = new ArrayList&lt;&gt;();</span><br><span class="line">           for (ColumnFamilyStore cfs : concatWithIndexes()) &#123;</span><br><span class="line">               if (forceSwitch || !cfs.getMemtableThreadSafe().isClean())</span><br><span class="line">                   icc.add(cfs);</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           final CountDownLatch latch = new CountDownLatch(icc.size());</span><br><span class="line">           for (ColumnFamilyStore cfs : icc) &#123;</span><br><span class="line">               // 生成新的memtable替换需要flush的memtable</span><br><span class="line">               Memtable memtable = cfs.data.switchMemtable();</span><br><span class="line">               if (memtable.isClean()) &#123;</span><br><span class="line">                   cfs.replaceFlushed(memtable, null);</span><br><span class="line">                   latch.countDown();</span><br><span class="line">               &#125; else &#123;</span><br><span class="line">                   logger.info(&quot;Enqueuing flush of &#123;&#125;&quot;, memtable);</span><br><span class="line">                   // 向flushWriter提交一个FlushRunnable</span><br><span class="line">                   memtable.flushAndSignal(latch, ctx);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           if (metric.memtableSwitchCount.count() == Long.MAX_VALUE)</span><br><span class="line">               metric.memtableSwitchCount.clear();</span><br><span class="line">           metric.memtableSwitchCount.inc();</span><br><span class="line"></span><br><span class="line">           // when all the memtables have been written, including for indexes, mark the flush in the commitlog header.</span><br><span class="line">           // a second executor makes sure the onMemtableFlushes get called in the right order,</span><br><span class="line">           // while keeping the wait-for-flush (future.get) out of anything latency-sensitive.</span><br><span class="line">           return postFlushExecutor.submit(new WrappedRunnable() &#123;</span><br><span class="line">               public void runMayThrow() throws InterruptedException, ExecutionException &#123;</span><br><span class="line">                   latch.await(); // 等待所有的flush完成</span><br><span class="line"></span><br><span class="line">                   if (!icc.isEmpty()) &#123;</span><br><span class="line">                       for (SecondaryIndex index : indexManager.getIndexesNotBackedByCfs()) &#123;</span><br><span class="line">                           // flush any non-cfs backed indexes</span><br><span class="line">                           logger.info(&quot;Flushing SecondaryIndex &#123;&#125;&quot;, index);</span><br><span class="line">                           index.forceBlockingFlush();</span><br><span class="line">                       &#125;</span><br><span class="line">                   &#125;</span><br><span class="line"></span><br><span class="line">                   if (writeCommitLog) &#123;</span><br><span class="line">                       // if we&#x27;re not writing to the commit log, we are replaying the log, so marking</span><br><span class="line">                       // the log header with &quot;you can discard anything written before the context&quot; is not valid</span><br><span class="line">                       CommitLog.instance.discardCompletedSegments(metadata.cfId, ctx.get());</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;);</span><br><span class="line">       &#125; finally &#123;</span><br><span class="line">           Keyspace.switchLock.writeLock().unlock();</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>接着看下FlushRunnable中的主要逻辑如下，简单来说就是创建一个SSTableWriter实例，然后循环所有的记录，调用append()方法进行写入，写入完成后再打开一个SSTableReader，用于后续对该SSTable的读取操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">protected void runMayThrow() throws Exception &#123;</span><br><span class="line">    long writeSize = getExpectedWriteSize();</span><br><span class="line">    // 获取有足够空间的目录</span><br><span class="line">    Directories.DataDirectory dataDirectory = getWriteDirectory(writeSize);</span><br><span class="line">    // 获取dataDirectory下保存sstable的子目录</span><br><span class="line">    File sstableDirectory = cfs.directories.getLocationForDisk(dataDirectory);</span><br><span class="line">    assert sstableDirectory != null : &quot;Flush task is not bound to any disk&quot;;</span><br><span class="line">    // 生成sstable</span><br><span class="line">    SSTableReader sstable = writeSortedContents(context, sstableDirectory);</span><br><span class="line">        cfs.replaceFlushed(Memtable.this, sstable);</span><br><span class="line">    latch.countDown();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private SSTableReader writeSortedContents(Future&lt;ReplayPosition&gt; context, File sstableDirectory)</span><br><span class="line">        throws ExecutionException, InterruptedException &#123;</span><br><span class="line">    logger.info(&quot;Writing &quot; + Memtable.this.toString());</span><br><span class="line">    SSTableReader ssTable;</span><br><span class="line">    // errors when creating the writer that may leave empty temp files.</span><br><span class="line">    // 生成sstable的临时路径，创建SSTableWriter，sstable的路径名的形式是ks-cf(-tmp)-version-fileIndex-component.db</span><br><span class="line">    SSTableWriter writer = createFlushWriter(cfs.getTempSSTablePath(sstableDirectory));</span><br><span class="line">    try &#123;</span><br><span class="line">        // (we can&#x27;t clear out the map as-we-go to free up memory,</span><br><span class="line">        //  since the memtable is being used for queries in the &quot;pending flush&quot; category)</span><br><span class="line">        for (Map.Entry&lt;RowPosition, AtomicSortedColumns&gt; entry : rows.entrySet()) &#123;</span><br><span class="line">            ColumnFamily cf = entry.getValue();</span><br><span class="line">            if (cf.isMarkedForDelete()) &#123; // 已经标记为要删除</span><br><span class="line">                // When every node is up, there&#x27;s no reason to write batchlog data out to sstables</span><br><span class="line">                // (which in turn incurs cost like compaction) since the BL write + delete cancel each other out,</span><br><span class="line">                // and BL data is strictly local, so we don&#x27;t need to preserve tombstones for repair.</span><br><span class="line">                // If we have a data row + row level tombstone, then writing it is effectively an expensive no-op so we skip it.</span><br><span class="line">                // See CASSANDRA-4667.</span><br><span class="line">                if (cfs.name.equals(SystemKeyspace.BATCHLOG_CF) &amp;&amp; cfs.keyspace.getName().equals(Keyspace.SYSTEM_KS)</span><br><span class="line">                        &amp;&amp; !(cf.getColumnCount() == 0))</span><br><span class="line">                    continue;</span><br><span class="line">                // TODO ???</span><br><span class="line">                // Pedantically, you could purge column level tombstones that are past GcGRace when writing to the SSTable.</span><br><span class="line">                // But it can result in unexpected behaviour where deletes never make it to disk, as they are lost</span><br><span class="line">                    // and so cannot override existing column values. So we only remove deleted columns if there</span><br><span class="line">                // is a CF level tombstone to ensure the delete makes it into an SSTable.</span><br><span class="line">                    // We also shouldn&#x27;t be dropping any columns obsoleted by partition and/or range tombstones in case</span><br><span class="line">                // the table has secondary indexes, or else the stale entries wouldn&#x27;t be cleaned up during compaction,</span><br><span class="line">                // and will only be dropped during 2i query read-repair, if at all.</span><br><span class="line">                if (!cfs.indexManager.hasIndexes())</span><br><span class="line">                    // 移除删除的列，或者schema中被drop的列</span><br><span class="line">                    currentSize.addAndGet(-ColumnFamilyStore.removeDeletedColumnsOnly(cf, Integer.MIN_VALUE));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (cf.getColumnCount() &gt; 0 || cf.isMarkedForDelete())</span><br><span class="line">                writer.append((DecoratedKey) entry.getKey(), cf);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (writer.getFilePointer() &gt; 0) &#123; //filePoint是当前写入的位置</span><br><span class="line">            ssTable = writer.closeAndOpenReader(); // 打开SSTable Reader</span><br><span class="line">            logger.info(String.format(&quot;Completed flushing %s (%d bytes) for commitlog position %s&quot;,</span><br><span class="line">                    ssTable.getFilename(), new File(ssTable.getFilename()).length(), context.get()));</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            writer.abort();</span><br><span class="line">            ssTable = null;</span><br><span class="line">            logger.info(&quot;Completed flushing; nothing needed to be retained.  Commitlog position was &#123;&#125;&quot;, context.get());</span><br><span class="line">        &#125;</span><br><span class="line">        return ssTable;</span><br><span class="line">    &#125; catch (Throwable e) &#123;</span><br><span class="line">        writer.abort();</span><br><span class="line">        throw Throwables.propagate(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>现在的重点来到了SSTableWriter类，先看下类中的重要属性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private IndexWriter iwriter; // 负责索引文件的写入</span><br><span class="line">  private SegmentedFile.Builder dbuilder;</span><br><span class="line">  private final SequentialWriter dataFile; // 负责sstable数据的写入</span><br><span class="line">  private DecoratedKey lastWrittenKey; // 最后一个写入的key</span><br><span class="line">  private FileMark dataMark;</span><br><span class="line">  private final SSTableMetadata.Collector sstableMetadataCollector; // 记录和统计元数据</span><br></pre></td></tr></table></figure>
<p>接着看主要的append()方法以及其中的rawAppend()方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">public void append(DecoratedKey decoratedKey, ColumnFamily cf) &#123;</span><br><span class="line">       if (decoratedKey.key.remaining() &gt; FBUtilities.MAX_UNSIGNED_SHORT) &#123;</span><br><span class="line">           logger.error(&quot;Key size &#123;&#125; exceeds maximum of &#123;&#125;, skipping row&quot;,</span><br><span class="line">                   decoratedKey.key.remaining(),</span><br><span class="line">                   FBUtilities.MAX_UNSIGNED_SHORT);</span><br><span class="line">           return;</span><br><span class="line">       &#125;</span><br><span class="line">       // 检查是否满足写入数据是有序的,获取当前写入位置</span><br><span class="line">       long startPosition = beforeAppend(decoratedKey);</span><br><span class="line">       try &#123;</span><br><span class="line">           RowIndexEntry entry = rawAppend(cf, startPosition, decoratedKey, dataFile.stream);</span><br><span class="line">           afterAppend(decoratedKey, startPosition, entry);</span><br><span class="line">       &#125; catch (IOException e) &#123;</span><br><span class="line">           throw new FSWriteError(e, dataFile.getPath());</span><br><span class="line">       &#125;</span><br><span class="line">       // 更新元数据中的统计数据</span><br><span class="line">       sstableMetadataCollector.update(dataFile.getFilePointer() - startPosition, cf.getColumnStats());</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   public static RowIndexEntry rawAppend(ColumnFamily cf, long startPosition,</span><br><span class="line">                                         DecoratedKey key, DataOutput out) throws IOException &#123;</span><br><span class="line">       // 要么有列内容，或者是个tombstone</span><br><span class="line">       assert cf.getColumnCount() &gt; 0 || cf.isMarkedForDelete();</span><br><span class="line"></span><br><span class="line">       // build()中完成了列的写入，并返回了列索引</span><br><span class="line">       ColumnIndex.Builder builder = new ColumnIndex.Builder(cf, key.key, out);</span><br><span class="line">       ColumnIndex index = builder.build(cf);</span><br><span class="line"></span><br><span class="line">       out.writeShort(END_OF_ROW);</span><br><span class="line">       return RowIndexEntry.create(startPosition, cf.deletionInfo().getTopLevelDeletion(), index);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   	// ColumnIndex的build方法</span><br><span class="line">  		public ColumnIndex build(ColumnFamily cf) throws IOException &#123;</span><br><span class="line">           // cf has disentangled the columns and range tombstones, we need to re-interleave them in comparator order</span><br><span class="line">           Iterator&lt;RangeTombstone&gt; rangeIter = cf.deletionInfo().rangeIterator();</span><br><span class="line">           RangeTombstone tombstone = rangeIter.hasNext() ? rangeIter.next() : null;</span><br><span class="line">           Comparator&lt;ByteBuffer&gt; comparator = cf.getComparator();</span><br><span class="line"></span><br><span class="line">           for (Column c : cf) &#123;</span><br><span class="line">               // 按照名称顺序写入，如果RangeTombstone.min在前，则先写RangeTombstone</span><br><span class="line">               while (tombstone != null &amp;&amp; comparator.compare(c.name(), tombstone.min) &gt;= 0) &#123;</span><br><span class="line">                   add(tombstone);</span><br><span class="line">                   tombstone = rangeIter.hasNext() ? rangeIter.next() : null;</span><br><span class="line">               &#125;</span><br><span class="line">               add(c);</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           while (tombstone != null) &#123;</span><br><span class="line">               add(tombstone);</span><br><span class="line">               tombstone = rangeIter.hasNext() ? rangeIter.next() : null;</span><br><span class="line">           &#125;</span><br><span class="line">           ColumnIndex index = build();</span><br><span class="line"></span><br><span class="line">           maybeWriteEmptyRowHeader();</span><br><span class="line"></span><br><span class="line">           return index;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	// ColumnIndex的add方法</span><br><span class="line">       public void add(OnDiskAtom column) throws IOException &#123;</span><br><span class="line">           atomCount++;</span><br><span class="line"></span><br><span class="line">           if (firstColumn == null) &#123;</span><br><span class="line">               firstColumn = column;</span><br><span class="line">               startPosition = endPosition;</span><br><span class="line">               // TODO: have that use the firstColumn as min + make sure we optimize that on read</span><br><span class="line">               // 计算indexBlock开头需要写入的RangeTombstone，并返回写入的大小</span><br><span class="line">               endPosition += tombstoneTracker.writeOpenedMarker(firstColumn, output, atomSerializer);</span><br><span class="line">               blockSize = 0; // We don&#x27;t count repeated tombstone marker in the block size, to avoid a situation</span><br><span class="line">               // where we wouldn&#x27;t make any progress because a block is filled by said marker</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           long size = column.serializedSizeForSSTable();</span><br><span class="line">           endPosition += size;</span><br><span class="line">           blockSize += size;</span><br><span class="line"></span><br><span class="line">           // if we hit the column index size that we have to index after, go ahead and index it.</span><br><span class="line">           // 当达到column_index_size_in_kb大小生成一个IndexInfo，同时开始一个新的block</span><br><span class="line">           if (blockSize &gt;= DatabaseDescriptor.getColumnIndexSize()) &#123;</span><br><span class="line">               IndexHelper.IndexInfo cIndexInfo = new IndexHelper.IndexInfo(firstColumn.name(),</span><br><span class="line">                       column.name(), indexOffset + startPosition, endPosition - startPosition);</span><br><span class="line">               result.columnsIndex.add(cIndexInfo);</span><br><span class="line">               firstColumn = null;</span><br><span class="line">               lastBlockClosing = column;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           // 写入rowHeader,包括key和行级别的deleteInfo</span><br><span class="line">           maybeWriteRowHeader();</span><br><span class="line"></span><br><span class="line">           // 写入column</span><br><span class="line">           atomSerializer.serializeForSSTable(column, output);</span><br><span class="line"></span><br><span class="line">           // TODO: Should deal with removing unneeded tombstones</span><br><span class="line">           // 移除不再会和后面的列有交集的RangeTombStone</span><br><span class="line">           tombstoneTracker.update(column, false);</span><br><span class="line"></span><br><span class="line">           lastColumn = column;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>
<p>在rawAppend()方法中出现了ColumnIndex类，顾名思义，该类表示给列加索引。因为Cassandra中的一行允许有很多列（最多2亿），显然这么多列在没有索引的情况下读取会很慢。在写入一行记录的时候会给column做索引，而Column索引（也称为promoted index）是每隔column_index_size_in_kb大小（称为IndexBolck）创建一个索引（并不是每隔几列，因为磁盘的读取是以Block为单位的），用IndexHelper.IndexInfo表示，如下图所示（图片来自参考文章2）。索引中包括该index block的第一个column和最后一个column。column index使得我们可以跳过前面的block来直接读取某些列所在的block，减少了IO操作的次数。此外我们需要在每个block的开头写入和该block相交的所有rangTombstone（表示列名落在该区间的列都被打上了删除标记），比如某个rangeTombstone的覆盖的区间很大，包含了当前block在内的多个block，那么该rangeTombstone是保存在block之前的，而要只读取当前block又需要知道该block的所有相交的rangeTombstone，就需要在写入的时候，把rangeTomestone也写入当前block。rangeTombstone和column都是OnDiskAtom，从这里可以看出OnDiskAtom并不是严格有序的，但是column是严格有序的，但这并不会造成任何问题。<br><img src="column_index.png" alt="column index"></p>
<p>rawAppend()方法最后返回了RowIndexEntry实例，其中记录了该行数据在sstable的起始位置以及column index，之后会调用afterAppend()方法把RowIndexEntry写入索引文件，其中主要调用了IndexWriter的append()方法，其中除了把key和对应的索引项写入索引文件，还每隔indexInterval项，向IndexSummaryBuilder中写入索引项的key和在索引文件中的位置。IndexSummary相当于是索引的索引，即整个索引是两层索引：底层是数据记录的索引，一行记录一个索引，是稠密索引；上层是稀疏索引，每隔indexInterval项，给记录索引创建一个索引。indexInterval默认是128，如果记录数增多，那么indexSummary中的项也会增大，但是indexSummary需要常驻内存，所以不能无限增长，默认最多项目是Integer.MAX_VALUE，超过了限制就会增大interval。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">// 追加index项到index文件</span><br><span class="line">   public void append(DecoratedKey key, RowIndexEntry indexEntry) &#123;</span><br><span class="line">   	// 向bloom filter写入数据</span><br><span class="line">       bf.add(key.key);</span><br><span class="line">       // 索引文件当前的写入位置</span><br><span class="line">       long indexPosition = indexFile.getFilePointer();</span><br><span class="line">       try &#123;</span><br><span class="line">       	// 写入key</span><br><span class="line">           ByteBufferUtil.writeWithShortLength(key.key, indexFile.stream);</span><br><span class="line">           // 写入索引项</span><br><span class="line">           RowIndexEntry.serializer.serialize(indexEntry, indexFile.stream);</span><br><span class="line">       &#125; catch (IOException e) &#123;</span><br><span class="line">           throw new FSWriteError(e, indexFile.getPath());</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">          	</span><br><span class="line">       summary.maybeAddEntry(key, indexPosition);</span><br><span class="line">       builder.addPotentialBoundary(indexPosition);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   // IndexSummaryBuilder的maybeAddEntry()</span><br><span class="line">   public IndexSummaryBuilder maybeAddEntry(DecoratedKey decoratedKey, long indexPosition) &#123;</span><br><span class="line">   	// 每隔indexInterval写入key和在索引文件中的位置。</span><br><span class="line">       if (keysWritten % indexInterval == 0) &#123;</span><br><span class="line">           byte[] key = ByteBufferUtil.getArray(decoratedKey.key);</span><br><span class="line">           keys.add(key);</span><br><span class="line">           offheapSize += key.length;</span><br><span class="line">           positions.add(indexPosition);</span><br><span class="line">           offheapSize += TypeSizes.NATIVE.sizeof(indexPosition);</span><br><span class="line">       &#125;</span><br><span class="line">       keysWritten++;</span><br><span class="line"></span><br><span class="line">       return this;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>IndexSummary是保存在堆外内存中的，具体的内存布局在IndexSummaryBuilder的build()方法中。 首先整个大小为 所有key的长度总和 + 表示indexFile中位置的long型pisition变量 + key.size() * 4，其中最后为key.size()个整数，表示IndexSummary项在该段内存中的偏移量，如下图所示。根据前后两个偏移量就可以知道一个IndexSummary项的长度，由于Position是long类型，固定为8个字节，因此可以计算出key的长度，所以写入的时候虽然key是变长的，但是并没有key的长度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public IndexSummary build(IPartitioner partitioner) &#123;</span><br><span class="line">    assert keys != null &amp;&amp; keys.size() &gt; 0;</span><br><span class="line">    assert keys.size() == positions.size();</span><br><span class="line"></span><br><span class="line">    Memory memory = Memory.allocate(offheapSize + (keys.size() * 4));</span><br><span class="line">    int idxPosition = 0;</span><br><span class="line">    int keyPosition = keys.size() * 4;</span><br><span class="line">    for (int i = 0; i &lt; keys.size(); i++) &#123;</span><br><span class="line">        // 写入每一项在memory的偏移量 </span><br><span class="line">        memory.setInt(idxPosition, keyPosition);</span><br><span class="line">        idxPosition += TypeSizes.NATIVE.sizeof(keyPosition);</span><br><span class="line"></span><br><span class="line">        byte[] temp = keys.get(i);</span><br><span class="line">        memory.setBytes(keyPosition, temp, 0, temp.length);</span><br><span class="line">        keyPosition += temp.length;</span><br><span class="line">        long tempPosition = positions.get(i);</span><br><span class="line">        memory.setLong(keyPosition, tempPosition);</span><br><span class="line">        keyPosition += TypeSizes.NATIVE.sizeof(tempPosition);</span><br><span class="line">    &#125;</span><br><span class="line">    return new IndexSummary(partitioner, memory, keys.size(), indexInterval);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="index_summary.png" alt="column index"></p>
<p>至此，一条记录的append过程处理完毕。</p>
<p>Reference:</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://thelastpickle.com/blog/2011/05/04/How-are-Memtables-measured.html">https://thelastpickle.com/blog/2011/05/04/How-are-Memtables-measured.html</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zqhxuyuan.github.io/2016/10/19/Cassandra-Code-StorageEngine/">https://zqhxuyuan.github.io/2016/10/19/Cassandra-Code-StorageEngine/</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://distributeddatastore.blogspot.com/2013/08/cassandra-sstable-storage-format.html">http://distributeddatastore.blogspot.com/2013/08/cassandra-sstable-storage-format.html</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/34570367/cassandra-3-0-updated-sstable-format">https://stackoverflow.com/questions/34570367/cassandra-3-0-updated-sstable-format</a></p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/09/15/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%943-SSTable%E7%9A%84%E5%86%99%E5%85%A5/" data-id="ckzldzrat000vtofw8yao303v" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Cassandra源码阅读随笔2-INSERT的执行-Memtable和CommitLog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/11/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%942-INSERT%E7%9A%84%E6%89%A7%E8%A1%8C-Memtable%E5%92%8CCommitLog/" class="article-date">
  <time datetime="2019-09-11T06:34:05.000Z" itemprop="datePublished">2019-09-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/11/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%942-INSERT%E7%9A%84%E6%89%A7%E8%A1%8C-Memtable%E5%92%8CCommitLog/">Cassandra源码阅读随笔2. INSERT的执行, Memtable和CommitLog</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="insert语句的解析"><a href="#insert语句的解析" class="headerlink" title="insert语句的解析"></a>insert语句的解析</h2><p>在前文创建的users表中插入一条记录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO users(id, age, user_name) VALUES (1, 8, &#x27;tom&#x27;);</span><br></pre></td></tr></table></figure>
<p>cql语句首先解析生成一个UpdateStatement的静态内部类ParsedInsert对象，其中主要保存了字段名和字段值，其中字段值是Constants$Literal对象，也就是字面常量，实际上就是保存了字符串形式的值以及对应的类型。常量的解析以及支持的类型可以从语法文件中找到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">constant returns [Constants.Literal constant]</span><br><span class="line">    : t=STRING_LITERAL &#123; $constant = Constants.Literal.string($t.text); &#125;</span><br><span class="line">    | t=INTEGER        &#123; $constant = Constants.Literal.integer($t.text); &#125;</span><br><span class="line">    | t=FLOAT          &#123; $constant = Constants.Literal.floatingPoint($t.text); &#125;</span><br><span class="line">    | t=BOOLEAN        &#123; $constant = Constants.Literal.bool($t.text); &#125;</span><br><span class="line">    | t=UUID           &#123; $constant = Constants.Literal.uuid($t.text); &#125;</span><br><span class="line">    | t=HEXNUMBER      &#123; $constant = Constants.Literal.hex($t.text); &#125;</span><br><span class="line">    | &#123; String sign=&quot;&quot;; &#125; (&#x27;-&#x27; &#123;sign = &quot;-&quot;; &#125; )? t=(K_NAN | K_INFINITY) &#123; $constant = Constants.Literal.floatingPoint(sign + $t.text); &#125;</span><br><span class="line">    ;</span><br></pre></td></tr></table></figure>
<p>在prepare()之后会生成一个UpdateStatement对象，UpdateStatement继承自ModificationStatement，同时用来表示insert update和delete操作，用StatementType区分不同的操作。在prepare()过程中会检测提交的值的类型和columnfamily元数据中的对应的列的类型是否兼容，而关于列的信息会保存在processedKeys和columnOperations中，其中前者保存的partition key和clustering key所在的列，而后面保存的是对于普通列的操作。以上面的insert语句为例，id和age保存在processedKeys，Restriction表示列上面的约束条件，此处都是表示相等的EQ；user_name保存到columnOperations，具体的是Operation的子类Constant$Setter，表示设置一个列的值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">protected final Map&lt;ColumnIdentifier, Restriction&gt; processedKeys = new HashMap&lt;&gt;();</span><br><span class="line">private final List&lt;Operation&gt; columnOperations = new ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure>

<p>生成UpdateStatement对象之后，调用execute()执行操作，主要逻辑在executeWithoutCondition()方法中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private ResultMessage executeWithoutCondition(QueryState queryState, QueryOptions options)</span><br><span class="line">        throws RequestExecutionException, RequestValidationException &#123;</span><br><span class="line">    ConsistencyLevel cl = options.getConsistency();</span><br><span class="line">    if (isCounter())</span><br><span class="line">        cl.validateCounterForWrite(cfm);</span><br><span class="line">    else</span><br><span class="line">        cl.validateForWrite(cfm.ksName);</span><br><span class="line"></span><br><span class="line">    // 生成Mutation</span><br><span class="line">    Collection&lt;? extends IMutation&gt; mutations = getMutations(options.getValues(), false, cl, queryState.getTimestamp());</span><br><span class="line">    // 应用Mutation</span><br><span class="line">    if (!mutations.isEmpty())</span><br><span class="line">        StorageProxy.mutateWithTriggers(mutations, cl, false);</span><br><span class="line"></span><br><span class="line">    return null;</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure>
<p>getMutations()方法如下，其中：</p>
<ol>
<li>buildPartitionKeyNames(): 把partition key所在列的值连接在一起，写入到一个ByteBuffer中。对于复合的partion key，允许在最后一列使用IN操作符，所以返回的是一个List。<br>比如现在某个表的partion key由a,b两列组成，update语句的where条件必须包含所有的primary key，其中最后一列可以用IN操作，那么<code>update some_cf set ... where a = &#39;a1&#39; and b in (&#39;b1, b2&#39;)</code>这种语句是合法的。<br>此时buildPartitionKeyNames返回的值就是 (‘a1’, ‘b1’)和(‘a1’, ‘b2’)。</li>
<li>createClusteringPrefixBuilder: 和buildPartitionKeyNames()类似</li>
<li>创建UpdateParameters对象，该对象中prefetchedLists表示当前要操作的记录。比如对于<code>update some_cf set some_column = some_column + 1 where ... </code>这样的更新操作，就需要先把要操作的记录读取出来。对于插入操作，获取不需要先读取的更新操作，prefetchedLists就为空。</li>
<li>构建RowMutation：在循环中构建行记录的变更。此处一个ColumnFamily实例表示一条记录。addUpdateForKey构建了列的column_name和column_value。其中需要注意的column_name不单单是schema定义中的列名，而是会把clustering key作为前缀和原来的列名组合在一起。比如我们插入记录(1, 8, ‘tom’)时，生成的cf中有2列（不包括partion key），键值对分别是(8: ), (8+user_name: ‘tome’)，其中clustering key所在列直接把值作为列名。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private Collection&lt;? extends IMutation&gt; getMutations(List&lt;ByteBuffer&gt; variables, boolean local, ConsistencyLevel cl, long now)</span><br><span class="line">        throws RequestExecutionException, RequestValidationException &#123;</span><br><span class="line">    List&lt;ByteBuffer&gt; keys = buildPartitionKeyNames(variables);</span><br><span class="line">    ColumnNameBuilder clusteringPrefix = createClusteringPrefixBuilder(variables);</span><br><span class="line"></span><br><span class="line">    UpdateParameters params = makeUpdateParameters(keys, clusteringPrefix, variables, local, cl, now);</span><br><span class="line"></span><br><span class="line">    Collection&lt;IMutation&gt; mutations = new ArrayList&lt;&gt;();</span><br><span class="line">    for (ByteBuffer key : keys) &#123;</span><br><span class="line">        ColumnFamily cf = UnsortedColumns.factory.create(cfm);</span><br><span class="line">        addUpdateForKey(cf, key, clusteringPrefix, params);</span><br><span class="line">        RowMutation rm = new RowMutation(cfm.ksName, key, cf);</span><br><span class="line">        mutations.add(isCounter() ? new CounterMutation(rm, cl) : rm);</span><br><span class="line">    &#125;</span><br><span class="line">    return mutations;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="insert执行"><a href="#insert执行" class="headerlink" title="insert执行"></a>insert执行</h2><p>生成了mutation之后就是执行了，statement的execute()方法中会调用方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public static void mutateWithTriggers(Collection&lt;? extends IMutation&gt; mutations,</span><br><span class="line">                                      ConsistencyLevel consistencyLevel,</span><br><span class="line">                                      boolean mutateAtomically)</span><br><span class="line">        throws WriteTimeoutException, UnavailableException, OverloadedException, InvalidRequestException &#123;</span><br><span class="line"></span><br><span class="line">    // 触发器应用mutation</span><br><span class="line">    Collection&lt;RowMutation&gt; augmented = TriggerExecutor.instance.execute(mutations);</span><br><span class="line"></span><br><span class="line">    if (augmented != null)</span><br><span class="line">        mutateAtomically(augmented, consistencyLevel);</span><br><span class="line">    else if (mutateAtomically)</span><br><span class="line">        mutateAtomically((Collection&lt;RowMutation&gt;) mutations, consistencyLevel);</span><br><span class="line">    else</span><br><span class="line">        mutate(mutations, consistencyLevel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于例子中没有定义触发器，那么直接进入 mutate(mutations, consistencyLevel)，其中主要调用了StorageProxy的performWrite()方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public static AbstractWriteResponseHandler performWrite(IMutation mutation,</span><br><span class="line">                                                        ConsistencyLevel consistency_level,</span><br><span class="line">                                                        String localDataCenter,</span><br><span class="line">                                                        WritePerformer performer,</span><br><span class="line">                                                        Runnable callback,</span><br><span class="line">                                                        WriteType writeType)</span><br><span class="line">        throws UnavailableException, OverloadedException &#123;</span><br><span class="line"></span><br><span class="line">    String keyspaceName = mutation.getKeyspaceName();</span><br><span class="line">    AbstractReplicationStrategy rs = Keyspace.open(keyspaceName).getReplicationStrategy();</span><br><span class="line"></span><br><span class="line">    Token tk = StorageService.getPartitioner().getToken(mutation.key());</span><br><span class="line"></span><br><span class="line">    // 根据复制策略和副本数量查找需要写入的节点</span><br><span class="line">    List&lt;InetAddress&gt; naturalEndpoints = StorageService.instance.getNaturalEndpoints(keyspaceName, tk);</span><br><span class="line">    Collection&lt;InetAddress&gt; pendingEndpoints =</span><br><span class="line">            StorageService.instance.getTokenMetadata().pendingEndpointsFor(tk, keyspaceName);</span><br><span class="line"></span><br><span class="line">    // 创建异步回调用的handler        </span><br><span class="line">    AbstractWriteResponseHandler responseHandler =</span><br><span class="line">            rs.getWriteResponseHandler(naturalEndpoints, pendingEndpoints, consistency_level, callback, writeType);</span><br><span class="line"></span><br><span class="line">    // 确定存活的节点数量满足consistencyLevel的要求</span><br><span class="line">    responseHandler.assureSufficientLiveNodes();</span><br><span class="line"></span><br><span class="line">    performer.apply(mutation, Iterables.concat(naturalEndpoints, pendingEndpoints), responseHandler,</span><br><span class="line">            localDataCenter, consistency_level);</span><br><span class="line">    return responseHandler;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="节点选择"><a href="#节点选择" class="headerlink" title="节点选择"></a>节点选择</h3><p>之后分析</p>
<h3 id="执行写入"><a href="#执行写入" class="headerlink" title="执行写入"></a>执行写入</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">public static void sendToHintedEndpoints(final RowMutation rm,</span><br><span class="line">                                         Iterable&lt;InetAddress&gt; targets,</span><br><span class="line">                                         AbstractWriteResponseHandler responseHandler,</span><br><span class="line">                                         String localDataCenter)</span><br><span class="line">        throws OverloadedException &#123;</span><br><span class="line">    // extra-datacenter replicas, grouped by dc</span><br><span class="line">    Map&lt;String, Collection&lt;InetAddress&gt;&gt; dcGroups = null;</span><br><span class="line">    // only need to create a Message for non-local writes</span><br><span class="line">    MessageOut&lt;RowMutation&gt; message = null;</span><br><span class="line"></span><br><span class="line">    for (InetAddress destination : targets) &#123;</span><br><span class="line">        // avoid OOMing due to excess hints.  we need to do this check even for &quot;live&quot; nodes, since we can</span><br><span class="line">        // still generate hints for those if it&#x27;s overloaded or simply dead but not yet known-to-be-dead.</span><br><span class="line">        // The idea is that if we have over maxHintsInProgress hints in flight, this is probably due to</span><br><span class="line">        // a small number of nodes causing problems, so we should avoid shutting down writes completely to</span><br><span class="line">        // healthy nodes.  Any node with no hintsInProgress is considered healthy.</span><br><span class="line">        // 如果有过多的未处理完hints直接抛异常</span><br><span class="line">        if (StorageMetrics.totalHintsInProgress.count() &gt; maxHintsInProgress</span><br><span class="line">                &amp;&amp; (getHintsInProgressFor(destination).get() &gt; 0 &amp;&amp; shouldHint(destination))) &#123;</span><br><span class="line">            throw new OverloadedException(&quot;Too many in flight hints: &quot; + StorageMetrics.totalHintsInProgress.count());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 检测目标节点是否存活    </span><br><span class="line">        if (FailureDetector.instance.isAlive(destination)) &#123;</span><br><span class="line">            if (destination.equals(FBUtilities.getBroadcastAddress()) &amp;&amp; OPTIMIZE_LOCAL_REQUESTS) &#123; // 本地操作</span><br><span class="line">                insertLocal(rm, responseHandler);</span><br><span class="line">            &#125; else &#123; // 发送给其他节点处理</span><br><span class="line">                // belongs on a different server</span><br><span class="line">                if (message == null)</span><br><span class="line">                    message = rm.createMessage();</span><br><span class="line">                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(destination);</span><br><span class="line">                // direct writes to local DC or old Cassandra versions</span><br><span class="line">                // (1.1 knows how to forward old-style String message IDs; updated to int in 2.0)</span><br><span class="line">                if (localDataCenter.equals(dc) || MessagingService.instance().getVersion(destination) &lt; MessagingService.VERSION_20) &#123;</span><br><span class="line">                    MessagingService.instance().sendRR(message, destination, responseHandler, true);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    Collection&lt;InetAddress&gt; messages = (dcGroups != null) ? dcGroups.get(dc) : null;</span><br><span class="line">                    if (messages == null) &#123;</span><br><span class="line">                        messages = new ArrayList&lt;&gt;(3); // most DCs will have &lt;= 3 replicas</span><br><span class="line">                        if (dcGroups == null)</span><br><span class="line">                            dcGroups = new HashMap&lt;&gt;();</span><br><span class="line">                        dcGroups.put(dc, messages);</span><br><span class="line">                    &#125;</span><br><span class="line">                    messages.add(destination);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123; // hintted off处理</span><br><span class="line">            if (!shouldHint(destination))</span><br><span class="line">                continue;</span><br><span class="line"></span><br><span class="line">            // Schedule a local hint</span><br><span class="line">            submitHint(rm, destination, responseHandler);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (dcGroups != null) &#123;</span><br><span class="line">        // for each datacenter, send the message to one node to relay the write to other replicas</span><br><span class="line">        if (message == null)</span><br><span class="line">            message = rm.createMessage();</span><br><span class="line"></span><br><span class="line">        for (Collection&lt;InetAddress&gt; dcTargets : dcGroups.values())</span><br><span class="line">            sendMessagesToNonlocalDC(message, dcTargets, responseHandler);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="insert的本地执行"><a href="#insert的本地执行" class="headerlink" title="insert的本地执行"></a>insert的本地执行</h4><p>insertLocal()方法几经辗转，最终调用的keyspace中的apply()方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public void apply(RowMutation mutation, boolean writeCommitLog, boolean updateIndexes) &#123;</span><br><span class="line">        // write the mutation to the commitlog and memtables</span><br><span class="line">        Tracing.trace(&quot;Acquiring switchLock read lock&quot;);</span><br><span class="line">        // siwtchLock是用于在切换memtable进行锁定的，普通读写只需加读锁</span><br><span class="line">        switchLock.readLock().lock(); </span><br><span class="line">        try &#123;</span><br><span class="line">            if (writeCommitLog) &#123;</span><br><span class="line">                Tracing.trace(&quot;Appending to commitlog&quot;);</span><br><span class="line">                // 写入commit log</span><br><span class="line">                CommitLog.instance.add(mutation);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            DecoratedKey key = StorageService.getPartitioner().decorateKey(mutation.key());</span><br><span class="line">            for (ColumnFamily cf : mutation.getColumnFamilies()) &#123;</span><br><span class="line">                ColumnFamilyStore cfs = columnFamilyStores.get(cf.id());</span><br><span class="line">                if (cfs == null) &#123;</span><br><span class="line">                    logger.error(&quot;Attempting to mutate non-existant column family &quot; + cf.id());</span><br><span class="line">                    continue;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                Tracing.trace(&quot;Adding to &#123;&#125; memtable&quot;, cf.metadata().cfName);</span><br><span class="line">                cfs.apply(key, cf, updateIndexes ? cfs.indexManager.updaterFor(key, cf) : SecondaryIndexManager.nullUpdater);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            switchLock.readLock().unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>keyspace中的apply()方法又调用了ColumnFamilyStore的apply()方法把数据写入memtable。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public void apply(DecoratedKey key, ColumnFamily columnFamily, SecondaryIndexManager.Updater indexer) &#123;</span><br><span class="line">    long start = System.nanoTime();</span><br><span class="line"></span><br><span class="line">    Memtable mt = getMemtableThreadSafe();</span><br><span class="line">    // 存入memtable</span><br><span class="line">    final long timeDelta = mt.put(key, columnFamily, indexer);</span><br><span class="line"></span><br><span class="line">    // 如果开启了rowCache则更新rowCache</span><br><span class="line">    maybeUpdateRowCache(key);</span><br><span class="line">    metric.writeLatency.addNano(System.nanoTime() - start);</span><br><span class="line">    if (timeDelta &lt; Long.MAX_VALUE)</span><br><span class="line">        metric.colUpdateTimeDeltaHistogram.update(timeDelta);</span><br><span class="line">    mt.maybeUpdateLiveRatio();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="hintted-handoff"><a href="#hintted-handoff" class="headerlink" title="hintted handoff"></a>hintted handoff</h4><p>之后分析</p>
<h2 id="CommitLog"><a href="#CommitLog" class="headerlink" title="CommitLog"></a>CommitLog</h2><p>从上面的写入过程中可以看出写入操作是写入到memetable的就返回了，如果这部分数据没有dump到磁盘，程序就退出了，那么内存中的数据就会丢失。为了在这种情况下，可以恢复数据，在写入memtable之前，需要先写入CommitLog。Cassandra的CommitLog类似于WAL日志，是保证数据持久性的一种手段。目前大部分基于LSM Tree的存储系统，一般都采取WAL+Memtable+SSTable的模式，只是具体的实现方式不同，比如Cassandra的SSTable是保存到本地磁盘的，而HBase得SSTable是保存到HDFS。</p>
<p>CommitLog是一个单例，在系统启动的时候实例化，commitlog被所有columnfamily所共享。CommitLog中重要属性如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 负责数据的写入和刷盘</span><br><span class="line">private final ICommitLogExecutorService executor;</span><br><span class="line"></span><br><span class="line">// 负责分配日志文件</span><br><span class="line">public final CommitLogAllocator allocator;</span><br><span class="line"></span><br><span class="line">public final CommitLogArchiver archiver = new CommitLogArchiver();</span><br><span class="line"></span><br><span class="line">// 当前正在使用的日志文件</span><br><span class="line">public volatile CommitLogSegment activeSegment;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再看下CommitLog的构造函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private CommitLog() &#123;</span><br><span class="line">    // 创建log文件的存储目录，默认是/var/lib/cassandra/commitlog</span><br><span class="line">    DatabaseDescriptor.createAllDirectories();</span><br><span class="line"></span><br><span class="line">    // 实例化CommitLogAllocator，并获得一个日志文件作为activeSegment</span><br><span class="line">    allocator = new CommitLogAllocator();</span><br><span class="line">    activateNextSegment();</span><br><span class="line"></span><br><span class="line">    // 根据配置文件实例化ICommitLogExecutorService的实现类</span><br><span class="line">    executor = DatabaseDescriptor.getCommitLogSync() == Config.CommitLogSync.batch</span><br><span class="line">            ? new BatchCommitLogExecutorService()</span><br><span class="line">            : new PeriodicCommitLogExecutorService(this);</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先解释下commitlog的大致原理：commit的日志是写入到固定大小的日志文件（用CommitLogSegment）中，当达到阈值之后会生成一个新的日志文件。日志的写入并不是实时写入或刷盘的，而是由ICommitLogExecutorService控制写入过程，根据配置条件进行刷盘。刷盘的策略有定期和批量，分别对应BatchCommitLogExecutorService和PeriodicCommitLogExecutorService，默认为定期刷盘。</p>
<p>下面来看下CommitLogSegment的重要属性，可以看到是基于MappedByteBuffer来实现写入的。此外还有一个cfLastWrite属性，其中保存了在该commit log中有过写入的columnfamily，同时这些数据还没dump到sstable。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class CommitLogSegment &#123;</span><br><span class="line">    // id的基础值，LogSegment的Id是在此基础上递增，每个log文件对应一个id</span><br><span class="line">    private final static long idBase = System.currentTimeMillis();</span><br><span class="line">    private final static AtomicInteger nextId = new AtomicInteger(1);</span><br><span class="line"></span><br><span class="line">    // The commit log entry overhead in bytes (int: length + long: head checksum + long: tail checksum)</span><br><span class="line">    static final int ENTRY_OVERHEAD_SIZE = 4 + 8 + 8;</span><br><span class="line"></span><br><span class="line">    // 保存相对于该LogSemgent是dirty的columnfamily，只有cfLastWrite才能删除或回收</span><br><span class="line">    // cache which cf is dirty in this segment to avoid having to lookup all ReplayPositions to decide if we can delete this segment</span><br><span class="line">    private final Map&lt;UUID, Integer&gt; cfLastWrite = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    public final long id;</span><br><span class="line"></span><br><span class="line">    private final File logFile;</span><br><span class="line">    private final RandomAccessFile logFileAccessor;</span><br><span class="line"></span><br><span class="line">    private boolean needsSync = false;</span><br><span class="line"></span><br><span class="line">    private final MappedByteBuffer buffer;</span><br><span class="line">    private final Checksum checksum;</span><br><span class="line">    private final DataOutputStream bufferStream;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>再来看CommitLogAllocator，CommitLogAllocator中有一个线程执行执行日志文件的分配，分配好未使用的日志文件存放在availableSegments，而当前正在写入和写完但还不能删除或回收的segment保存在activeSegments中。此外，系统中还配置了commitlog可以占用的最大空间（commitlog_total_space_in_mb），如果超过这个值，会调用flushOldestKeyspaces()方法来强制把activeSegments队首的CommitLogSegment中的cfLastWrite属性所包含的columnfamily的memtable刷到磁盘，依此来重用这个队首的这个logSegment。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">public class CommitLogAllocator &#123;</span><br><span class="line">   </span><br><span class="line">    // Segments that are ready to be used</span><br><span class="line">    private final BlockingQueue&lt;CommitLogSegment&gt; availableSegments = new LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    // Allocations to be run by the thread</span><br><span class="line">    private final BlockingQueue&lt;Runnable&gt; queue = new LinkedBlockingQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    // 正在写入的和写完但还不能删除或回收的segment</span><br><span class="line">    private final ConcurrentLinkedQueue&lt;CommitLogSegment&gt; activeSegments = new ConcurrentLinkedQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    // 保存所有CommitLogSegment的大小</span><br><span class="line">    private final AtomicLong size = new AtomicLong();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    private final Thread allocationThread;</span><br><span class="line">    private volatile boolean run = true;</span><br><span class="line"></span><br><span class="line">    public CommitLogAllocator() &#123;</span><br><span class="line">        // The run loop for the allocation thread</span><br><span class="line">        Runnable runnable = new WrappedRunnable() &#123;</span><br><span class="line">            public void runMayThrow() throws Exception &#123;</span><br><span class="line">                while (run) &#123;</span><br><span class="line">                    try &#123;</span><br><span class="line">                        Runnable r = queue.poll(TICK_CYCLE_TIME, TimeUnit.MILLISECONDS);</span><br><span class="line">                        if (r != null) &#123;</span><br><span class="line">                            r.run();</span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            if (availableSegments.isEmpty() &amp;&amp; (activeSegments.isEmpty() || createReserveSegments)) &#123;</span><br><span class="line">                                logger.debug(&quot;No segments in reserve; creating a fresh one&quot;);</span><br><span class="line">                                createFreshSegment();</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                    &#125; catch (Throwable t) &#123;</span><br><span class="line">                        if (!CommitLog.handleCommitError(&quot;Failed to allocate new commit log segments&quot;, t))</span><br><span class="line">                            return;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        allocationThread = new Thread(runnable, &quot;COMMIT-LOG-ALLOCATOR&quot;);</span><br><span class="line">        allocationThread.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">&#125;    </span><br></pre></td></tr></table></figure>

<p>现在来看写入过程中调用的CommitLog的add()方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public void add(RowMutation rm) &#123;</span><br><span class="line">    executor.add(new LogRecordAdder(rm));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该add()方法封装了LogRecordAdder对象提交给executore，LogRecordAdder继承了Runnable，run()方法中就是调用CommitLogSegment的write()方法实现对activeSegment的写入。</p>
<p>看下默认使用的PeriodicCommitLogExecutorService的实现，发现该类只有一个BlockingQueue来缓存要执行的操作，一个appendingThread来执行所有的IO操作，另一个线程定期向BlockingQueue中提交刷盘操作。我们发现并没有由执行insert操作的线程来执行commit log的写入，而是都交由一个appendingThread来完成，这样可以避免多同一个文件的多线程写入所需要的加锁等操作带来的影响。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class PeriodicCommitLogExecutorService implements ICommitLogExecutorService &#123;</span><br><span class="line"></span><br><span class="line">    // 保存要执行的Runnable，里面封装了对LogSegment的写入操作</span><br><span class="line">    private final BlockingQueue&lt;Runnable&gt; queue;</span><br><span class="line">    protected volatile long completedTaskCount = 0;</span><br><span class="line"></span><br><span class="line">    // 只能Runnable的线程</span><br><span class="line">    private final Thread appendingThread;</span><br><span class="line"></span><br><span class="line">    private volatile boolean run = true;</span><br><span class="line"></span><br><span class="line">    public PeriodicCommitLogExecutorService(final CommitLog commitLog) &#123;</span><br><span class="line">        queue = new LinkedBlockingQueue&lt;Runnable&gt;(DatabaseDescriptor.getCommitLogPeriodicQueueSize());</span><br><span class="line">        Runnable runnable = new WrappedRunnable() &#123;</span><br><span class="line">            public void runMayThrow() throws Exception &#123;</span><br><span class="line">                while (run) &#123;</span><br><span class="line">                    Runnable r = queue.poll(100, TimeUnit.MILLISECONDS);</span><br><span class="line">                    if (r == null)</span><br><span class="line">                        continue;</span><br><span class="line">                    r.run();</span><br><span class="line">                    completedTaskCount++;</span><br><span class="line">                &#125;</span><br><span class="line">                commitLog.sync();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        appendingThread = new Thread(runnable, &quot;COMMIT-LOG-WRITER&quot;);</span><br><span class="line">        appendingThread.start();</span><br><span class="line"></span><br><span class="line">        final Callable syncer = new Callable() &#123;</span><br><span class="line">            public Object call() throws Exception &#123;</span><br><span class="line">                commitLog.sync();</span><br><span class="line">                return null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        new Thread(new Runnable() &#123;</span><br><span class="line">            public void run() &#123;</span><br><span class="line">                while (run) &#123;</span><br><span class="line">                    try &#123;</span><br><span class="line">                        FBUtilities.waitOnFuture(submit(syncer));</span><br><span class="line">                        Uninterruptibles.sleepUninterruptibly(DatabaseDescriptor.getCommitLogSyncPeriod(), TimeUnit.MILLISECONDS);</span><br><span class="line">                    &#125; catch (Throwable t) &#123;</span><br><span class="line">                        ......</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, &quot;PERIODIC-COMMIT-LOG-SYNCER&quot;).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure>

<p>至此，还有一个问题没有明确，一个commit log何时对于一个column family来说是可以移除的。首先在写入的write()方法中会调用markDirty()方法在cfLastWrite中记录某个columnfamily在日志文件中的写入位置，随着写入的进行，该位置增大。当某个logSegment写满，使用新的logSement，位置被重置了，但是新的logSegment的id更大。因此从columnfamily的角度来看(segmentId, position)作为整体是递增的。那什么时候会清除这些记录呢？在ColumnFamilyStore的switchMemtable()方法中会调用CommitLog的discardCompletedSegments()方法，而该方法中会调用CommitLogSegment的markClean()方法来清除这些记录。</p>
<p>在switchMemtable时，会把当前的memtable dump到磁盘，那么该ColumnFamily的数据都持久化了，此时获取当前写入的日志文件和位置(currentSegmentId, currrentPostion)，然后遍历所有的active segment的cfLastWrite，如果有当前columnfamily的记录，且满足 segmentId &lt; currentSegmentId 或 segmentId == currentSegmentId &amp;&amp; position &lt; currrentPostion就可以清除了。如果一个logSegment的cfLastWrite为空，那么说明可以被回收或删除了。</p>
<h2 id="ColumnFamilyStore和Memtable"><a href="#ColumnFamilyStore和Memtable" class="headerlink" title="ColumnFamilyStore和Memtable"></a>ColumnFamilyStore和Memtable</h2><p>在前文中讲到，在创建columnfamily的过程中会生成一个对应的ColumnFamilyStore实例，在实例化ColumnFamilyStore的时候会创建一个Memtable。ColumnFamilyStore的data属性是一个DataTracker对象，DataTracker中View引用了ColumnFamilyStore中所有的memtable和sstable。</p>
<p>Memtable需要一个按照partition key排序的有序容器来保存记录，Cassandra使用ConcurrentSkipListMap来作为这个容器，对应的属性是rows。Cassandra中row的存储和直觉中的不一样，是一个partition key对应一个row，比如现在表定义的PRIMARY KEY是(k1, k2)，现在插入两条数据(v1, v2_1, …) 和(v1, v2_2)，这是两条数据，但是保存的时候是在一个row里面，每个row由很多个cell组成，cell是键值对，键是 clustering key + column name，而这些cell也是根据key有序排列的。</p>
<p>Memtable的put流程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">long put(DecoratedKey key, ColumnFamily columnFamily, SecondaryIndexManager.Updater indexer) &#123;</span><br><span class="line">    return resolve(key, columnFamily, indexer);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private long resolve(DecoratedKey key, ColumnFamily cf, SecondaryIndexManager.Updater indexer) &#123;</span><br><span class="line">    AtomicSortedColumns previous = rows.get(key);</span><br><span class="line"></span><br><span class="line">    if (previous == null) &#123;</span><br><span class="line">        AtomicSortedColumns empty = cf.cloneMeShallow(AtomicSortedColumns.factory, false);</span><br><span class="line">        // get之后为null, 还是需要用putIfAbsent，因为先get后put不是原子的。</span><br><span class="line">        // 先放一个空的行进去，在后面再把各个列填进去，以免putIfAbsent不成功，白费功夫</span><br><span class="line">        previous = rows.putIfAbsent(new DecoratedKey(key.token, allocator.clone(key.key)), empty);</span><br><span class="line">        if (previous == null)</span><br><span class="line">            previous = empty;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    final Pair&lt;Long, Long&gt; pair = previous.addAllWithSizeDelta(cf, allocator, localCopyFunction, indexer);</span><br><span class="line">    currentSize.addAndGet(pair.left);</span><br><span class="line">    currentOperations.addAndGet(cf.getColumnCount() + (cf.isMarkedForDelete() ? 1 : 0) + cf.deletionInfo().rangeCount());</span><br><span class="line">    return pair.right;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>memtable中保存的记录是用AtomicSortedColumns表示的，而还未添加memtable的记录则是一个UnSortedColumns。AtomicSortedColumns中的列是按照列名排列的，因为sstable中保存的数据也是有序的。排序的好处是可以作索引和二分查找，在列的数目很大的情况下查找某几列时可以大幅提高效率。AtomicSortedColumns是线程安全的，里面保存列的容器时SnapTree，使用copy-on-write来保证线程安全，此外操作是原子性，特别是添加多个列，详见下面的addAllWithSizeDelta的说明，该函数的返回值之一是该列在序列化之后大小的改变。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">public Pair&lt;Long, Long&gt; addAllWithSizeDelta(ColumnFamily cm, Allocator allocator,</span><br><span class="line">                                            Function&lt;Column, Column&gt; transformation, SecondaryIndexManager.Updater indexer) &#123;</span><br><span class="line">    /*</span><br><span class="line">     *  此处的操作需要保持原子性和隔离性。为了达到这个目的，先把原来保存各个列的map拷贝一份，</span><br><span class="line">     *  然后写入新的列。当所有的列写完采用CAS操作进行更新。在列数很多的情况下，</span><br><span class="line">     *  在所有列加入后的CAS阶段如果失败了，那么代价就太大了。为了减少这种情况的出现，</span><br><span class="line">     *  没处理完一列就检查一遍，这样可以尽早发现问题，避免不必要的操作。</span><br><span class="line">     */</span><br><span class="line">    Holder current, modified;</span><br><span class="line">    long sizeDelta;</span><br><span class="line">    long timeDelta;</span><br><span class="line"></span><br><span class="line">    main_loop:</span><br><span class="line">    do &#123;</span><br><span class="line">        sizeDelta = 0;</span><br><span class="line">        timeDelta = Long.MAX_VALUE;</span><br><span class="line">        current = ref.get();</span><br><span class="line">        DeletionInfo newDelInfo = current.deletionInfo;</span><br><span class="line">        if (cm.deletionInfo().mayModify(newDelInfo)) &#123;</span><br><span class="line">            newDelInfo = current.deletionInfo.copy().add(cm.deletionInfo());</span><br><span class="line">            sizeDelta += newDelInfo.dataSize() - current.deletionInfo.dataSize();</span><br><span class="line">        &#125;</span><br><span class="line">        modified = new Holder(current.map.clone(), newDelInfo);</span><br><span class="line"></span><br><span class="line">        for (Column column : cm) &#123;</span><br><span class="line">            final Pair&lt;Integer, Long&gt; pair = modified.addColumn(transformation.apply(column), allocator, indexer);</span><br><span class="line">            sizeDelta += pair.left;</span><br><span class="line"></span><br><span class="line">            //We will store the minimum delta for all columns if enabled</span><br><span class="line">            if (enableColUpdateTimeDelta)</span><br><span class="line">                timeDelta = Math.min(pair.right, timeDelta);</span><br><span class="line"></span><br><span class="line">            // bail early if we know we&#x27;ve been beaten</span><br><span class="line">            if (ref.get() != current)</span><br><span class="line">                continue main_loop;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; while (!ref.compareAndSet(current, modified));</span><br><span class="line"></span><br><span class="line">    indexer.updateRowLevelIndexes();</span><br><span class="line"></span><br><span class="line">    return Pair.create(sizeDelta, timeDelta);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/09/11/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%942-INSERT%E7%9A%84%E6%89%A7%E8%A1%8C-Memtable%E5%92%8CCommitLog/" data-id="ckzldzrat000utofw9483ab5d" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Cassandra源码阅读随笔1-CQL的执行和建表过程" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/04/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%941-CQL%E7%9A%84%E6%89%A7%E8%A1%8C%E5%92%8C%E5%BB%BA%E8%A1%A8%E8%BF%87%E7%A8%8B/" class="article-date">
  <time datetime="2019-09-04T01:31:07.000Z" itemprop="datePublished">2019-09-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/09/04/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%941-CQL%E7%9A%84%E6%89%A7%E8%A1%8C%E5%92%8C%E5%BB%BA%E8%A1%A8%E8%BF%87%E7%A8%8B/">Cassandra源码阅读随笔1. CQL的执行和建表过程</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="cql简介"><a href="#cql简介" class="headerlink" title="cql简介"></a>cql简介</h2><p>cql是Cassandra定义的类似于SQL的查询语言，可以打开Cassandra源码的bin目录下的cqlsh脚本。cql的执行大体上分为两个步骤：解析语句和执行语句。Cassandra中使用Antlr生成cql语句的lexer和parser。Antlr是一种自上而下的解析器，具体使用方法参考《Antlr权威指南》。cql语句解析后生成Statement对象，之后进行参数绑定，最后调用Statement对象的execute()方法。</p>
<p>对于cqlsh命令行中输入的cql语句，由CassandraServer类中的execute_cql3_query()方法处理。该方法首先从ByteBuffer中还原cql语句，然后交给QueryHandler处理。QueryHandler是一个接口，如果没有自定义该接口的实现类，那么进入默认的实现类QueryProcessor的process()方法。</p>
<p>process()方法中首先调用parseStatement()方法进行cql解析，返回ParsedStatement的子类对象。然后调用prepare()方法进行参数绑定，如果参数个数和值的个数不同，则抛出异常。之后在processStatement方法中分别调用statement对象的checkAcccess()和vaidate()方法进行访问控制和有效性校验，最后调用execute()方法执行语句。</p>
<h2 id="keyspace的创建"><a href="#keyspace的创建" class="headerlink" title="keyspace的创建"></a>keyspace的创建</h2><p>Cassandra中的keyspace是类似于mysql中的database的概念，可以通过cql语句来创建keyspace，比如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE KEYSPACE IF NOT EXISTS cql_demo WITH REPLICATION=&#123;&#x27;class&#x27;:&#x27;SimpleStrategy&#x27;,&#x27;replication_factor&#x27;:2&#125;;</span><br></pre></td></tr></table></figure>

<p>CREATE KEYSPACE语句会被解析成一个CreateKeyspaceStatement对象，在Cql.g中找到对应的语法生成式如下，可以看到主要是为了解析出keyspace的名称，以及相关的设置属性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">createKeyspaceStatement returns [CreateKeyspaceStatement expr]</span><br><span class="line">    @init &#123;</span><br><span class="line">        KSPropDefs attrs = new KSPropDefs();</span><br><span class="line">        boolean ifNotExists = false;</span><br><span class="line">    &#125;</span><br><span class="line">    : K_CREATE K_KEYSPACE (K_IF K_NOT K_EXISTS &#123; ifNotExists = true; &#125; )? ks=keyspaceName</span><br><span class="line">      K_WITH properties[attrs] &#123; $expr = new CreateKeyspaceStatement(ks, attrs, ifNotExists); &#125;</span><br><span class="line">    ;</span><br></pre></td></tr></table></figure>

<p>CreateKeyspaceStatement继承了抽象类SchemaAlteringStatement，该类的execute()方法主要调用了抽象方法announceMigration()。CreateKeyspaceStatement中的重写实现如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public boolean announceMigration() throws RequestValidationException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        MigrationManager.announceNewKeyspace(attrs.asKSMetadata(name));</span><br><span class="line">        return true;</span><br><span class="line">    &#125; catch (AlreadyExistsException e) &#123;</span><br><span class="line">        if (ifNotExists)</span><br><span class="line">            return false;</span><br><span class="line">        throw e;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到主要调用了MigrationManager类中的方法。首先会创建一个包含了keyspace元数据的KSMetaData对象，然后调用announceNewKeyspace()另外一个重载方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public static void announceNewKeyspace(KSMetaData ksm, long timestamp) throws ConfigurationException &#123;</span><br><span class="line">    ksm.validate();</span><br><span class="line"></span><br><span class="line">    if (Schema.instance.getKSMetaData(ksm.name) != null)</span><br><span class="line">        throw new AlreadyExistsException(ksm.name);</span><br><span class="line"></span><br><span class="line">    logger.info(String.format(&quot;Create new Keyspace: %s&quot;, ksm));</span><br><span class="line">    announce(ksm.toSchema(timestamp));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public RowMutation toSchema(long timestamp) &#123;</span><br><span class="line">    RowMutation rm = new RowMutation(Keyspace.SYSTEM_KS, SystemKeyspace.getSchemaKSKey(name)); // ks: system</span><br><span class="line">    ColumnFamily cf = rm.addOrGet(CFMetaData.SchemaKeyspacesCf);  // cf: schema_keyspaces</span><br><span class="line"></span><br><span class="line">    cf.addColumn(Column.create(durableWrites, timestamp, &quot;durable_writes&quot;));</span><br><span class="line">    cf.addColumn(Column.create(strategyClass.getName(), timestamp, &quot;strategy_class&quot;));</span><br><span class="line">    cf.addColumn(Column.create(json(strategyOptions), timestamp, &quot;strategy_options&quot;));</span><br><span class="line"></span><br><span class="line">    for (CFMetaData cfm : cfMetaData.values())</span><br><span class="line">        cfm.toSchema(rm, timestamp);</span><br><span class="line"></span><br><span class="line">    return rm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该方法中调用toSchema()方法生成了一个RowMutation对象，该对象代表了一条记录的变更。在Cassandra中，keyspace, columnfamily和 column的schema信息分别保存在SCHEMA_{KEYSPACES, COLUMNFAMILIES, COLUMNS}_CF表中。</p>
<p>此处我们新建一个keyspace，因此需要向SCHEMA_KEYSPACES_CF中加入一条记录。announce()方法中会把该变更操作作为runnable提交到MIGRATION stage，同时把keyspace的变更信息发送给其他存活的节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private static Future announce(final Collection&lt;RowMutation&gt; schema) &#123;</span><br><span class="line">    Future&lt;?&gt; f = StageManager.getStage(Stage.MIGRATION).submit(new WrappedRunnable() &#123;</span><br><span class="line">        protected void runMayThrow() throws IOException, ConfigurationException &#123;</span><br><span class="line">            DefsTables.mergeSchema(schema);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    for (InetAddress endpoint : Gossiper.instance.getLiveMembers()) &#123;</span><br><span class="line">        // only push schema to nodes with known and equal versions</span><br><span class="line">        if (!endpoint.equals(FBUtilities.getBroadcastAddress()) &amp;&amp;</span><br><span class="line">                MessagingService.instance().knowsVersion(endpoint) &amp;&amp;</span><br><span class="line">                MessagingService.instance().getRawVersion(endpoint) == MessagingService.current_version)</span><br><span class="line">            pushSchemaMutation(endpoint, schema);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     return f;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之后会调用该方法返回的future对象的get方法以等待操作完成。mergeSchema()方法的处理如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public static synchronized void mergeSchema(Collection&lt;RowMutation&gt; mutations) &#123;</span><br><span class="line">        // current state of the schema</span><br><span class="line">        // 获取当前keyspace和columnfamily的schema信息，就是扫描当前所有</span><br><span class="line">        Map&lt;DecoratedKey, ColumnFamily&gt; oldKeyspaces = SystemKeyspace.getSchema(SystemKeyspace.SCHEMA_KEYSPACES_CF);</span><br><span class="line">        Map&lt;DecoratedKey, ColumnFamily&gt; oldColumnFamilies = SystemKeyspace.getSchema(SystemKeyspace.SCHEMA_COLUMNFAMILIES_CF);</span><br><span class="line"></span><br><span class="line">        // 新建keyspace对象，并保存到Schema单例中</span><br><span class="line">        // 一个mutation一个记录写入相应的表（此处写入的是memtable），此处的表是SCHEMA_KEYSPACES_CF</span><br><span class="line">        for (RowMutation mutation : mutations)</span><br><span class="line">            mutation.apply();</span><br><span class="line"></span><br><span class="line">        // 存储schema相关的columnfamily的memtable强制刷盘</span><br><span class="line">        if (!StorageService.instance.isClientMode())</span><br><span class="line">            flushSchemaCFs();</span><br><span class="line"></span><br><span class="line">        // with new data applied</span><br><span class="line">        // 重新读取schema信息</span><br><span class="line">        Map&lt;DecoratedKey, ColumnFamily&gt; newKeyspaces = SystemKeyspace.getSchema(SystemKeyspace.SCHEMA_KEYSPACES_CF);</span><br><span class="line">        Map&lt;DecoratedKey, ColumnFamily&gt; newColumnFamilies = SystemKeyspace.getSchema(SystemKeyspace.SCHEMA_COLUMNFAMILIES_CF);</span><br><span class="line"></span><br><span class="line">        Set&lt;String&gt; keyspacesToDrop = mergeKeyspaces(oldKeyspaces, newKeyspaces);</span><br><span class="line">        mergeColumnFamilies(oldColumnFamilies, newColumnFamilies);</span><br><span class="line"></span><br><span class="line">        // it is safe to drop a keyspace only when all nested ColumnFamilies where deleted</span><br><span class="line">        // 清除要删掉的keyspace，比如使用了drop keyspace命令</span><br><span class="line">        for (String keyspaceToDrop : keyspacesToDrop)</span><br><span class="line">            dropKeyspace(keyspaceToDrop);</span><br><span class="line"></span><br><span class="line">        // 把schema相关的所有列信息用于计算md5值，然后用该值生成UUID作为版本号，然后写入系统表空间的local表中</span><br><span class="line">        Schema.instance.updateVersionAndAnnounce();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>至此keyspace的创建流程结束，总结起来就是在SCHEMA_KEYSPACES_CF表中写入一条记录（数据的写入在之后的章节中说明），并把相关的元数据保存到Schema单例中。</p>
<h2 id="columnfamily的创建"><a href="#columnfamily的创建" class="headerlink" title="columnfamily的创建"></a>columnfamily的创建</h2><p>在创建columnfamily通过会使用<code>use cql_demo</code>语句来切换到对应的keyspace，该语句的作用比较简单，就是对当前会话的clientState对象的keyspace属性进行赋值，并作为默认的keyspace。</p>
<p>和keysapce的创建类似，我们用cql语句来创建columnfamily :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE users (</span><br><span class="line">	id int,</span><br><span class="line">    age int, </span><br><span class="line">	user_name varchar,</span><br><span class="line">	PRIMARY KEY(id, age)</span><br><span class="line">);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>建表语句解析后生成CreateTableStatement的静态内部类RawStatement的对象，调用RawStatement的prepare()方法返回CreateTableStatement。CreateTableStatement的主要属性说明如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class CreateTableStatement extends SchemaAlteringStatement &#123;</span><br><span class="line"></span><br><span class="line">    // clustering key 对应的类型，如果是复合的，那么是CompositeType类型的。</span><br><span class="line">    // cassandra会给comparator都加上一个UTF8Type，所以例子中的建表语句的生成comparator就是复合类型(Int32Type, UTF8Type)</span><br><span class="line">    public AbstractType&lt;?&gt; comparator; </span><br><span class="line">    private AbstractType&lt;?&gt; defaultValidator;</span><br><span class="line">    </span><br><span class="line">    // partition key对应的类型，如果是复合的，那么是CompositeType类型的</span><br><span class="line">    private AbstractType&lt;?&gt; keyValidator; </span><br><span class="line"></span><br><span class="line">    private final List&lt;ByteBuffer&gt; keyAliases = new ArrayList&lt;&gt;();    // partition key</span><br><span class="line">    private final List&lt;ByteBuffer&gt; columnAliases = new ArrayList&lt;&gt;(); // clustering key</span><br><span class="line">    private ByteBuffer valueAlias;</span><br><span class="line"></span><br><span class="line">    private boolean isDense;</span><br><span class="line"></span><br><span class="line">    // 除了partition key和clustering key对应的类之外的其他列</span><br><span class="line">    private final Map&lt;ColumnIdentifier, AbstractType&gt; columns = new HashMap&lt;&gt;();</span><br><span class="line">    private final Set&lt;ColumnIdentifier&gt; staticColumns;</span><br><span class="line">    private final CFPropDefs properties;</span><br><span class="line">    private final boolean ifNotExists;</span><br><span class="line">    </span><br><span class="line">&#125;    </span><br></pre></td></tr></table></figure>
<p>CreateTableStatement的execute()执行路径和CreateKeyspaceStatement类似，主要进行了如下调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MigrationManager.announceNewColumnFamily(getCFMetaData())</span><br></pre></td></tr></table></figure>
<p>首先构建columnfamily的元数据CFMetaData。CFMetaData中主要保存了ksName, cfName，column的定义column_metadata，cf的类型cfType（Standard或Super），以及一些可选的属性等。其中column_metadata的每一列用ColumnDefinition表示，相比于CreateTableStatement中的ColumnIdentifier，ColumnDefinition的定义中多了indexType和indexName（二级索引secondaryIndex相关），此外还明确定义了列的类型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public enum Type &#123;</span><br><span class="line">    PARTITION_KEY,</span><br><span class="line">    CLUSTERING_KEY,</span><br><span class="line">    REGULAR,</span><br><span class="line">    COMPACT_VALUE,</span><br><span class="line">    STATIC</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>announceNewColumnFamily()方法中的主要逻辑如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">announce(addSerializedKeyspace(cfm.toSchema(FBUtilities.timestampMicros()), cfm.ksName));</span><br></pre></td></tr></table></figure>
<p>cfm.toSchema和ksm.toSchema的类似，也是生成一条保存了CFMetaData数据的schema_columnfamilies记录，addSerializedKeyspace()方法读取本地schema_keyspaces中关于该cf所属keyspace的相关信息，该方法调用主要是为了防止在集群中keysapce在某个节点创建后，keyspace还未传播到其他节点，而又在该节点执行创建该keyspace下的columnfamily的操作引起的错误。因为本地未获取到keyspace，addSerializedKeyspace中的assert会及时报错。</p>
<p>announce()方法的调用路径和创建keyspace时是一样的，唯一不同的是此时我们新建的是columnfamily，会调用addColumnFamily()方法来进行处理，其中最主要的方法是initCf()方法的调用，该方法中会创建ColumnFamilyStore对象（顾名思义，该类是用于管理columnfamily的存储，在后文中介绍），同时在Cassandra的数据目录中（默认是/var/lib/cassandra/data/）创建一个目录（cql_demo/users）用于存放该columnfamily的数据。由于目前并没有写入任务数据，所以目前users目录是空的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/09/04/Cassandra%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%941-CQL%E7%9A%84%E6%89%A7%E8%A1%8C%E5%92%8C%E5%BB%BA%E8%A1%A8%E8%BF%87%E7%A8%8B/" data-id="ckzldzra90001tofw5e5qgmdx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BookKeeper/" rel="tag">BookKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cassandra/" rel="tag">Cassandra</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parser-combinator/" rel="tag">Parser combinator</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pulasr/" rel="tag">Pulasr</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/BookKeeper/" style="font-size: 10px;">BookKeeper</a> <a href="/tags/Cassandra/" style="font-size: 20px;">Cassandra</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Parser-combinator/" style="font-size: 10px;">Parser combinator</a> <a href="/tags/Pulasr/" style="font-size: 10px;">Pulasr</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/07/Parser-combinator%E5%88%9D%E6%8E%A21-%E7%AE%80%E6%98%93JSON%E8%A7%A3%E6%9E%90%E5%99%A8/">Parser combinator初探1-简易JSON解析器</a>
          </li>
        
          <li>
            <a href="/2019/12/28/Cassandra%E7%AC%94%E8%AE%B01-Compaction%E7%AD%96%E7%95%A5/">Cassandra笔记1-Compaction策略</a>
          </li>
        
          <li>
            <a href="/2019/11/27/HBase%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E9%9A%8F%E7%AC%942-MemStore/">HBase源码阅读随笔2-MemStore</a>
          </li>
        
          <li>
            <a href="/2019/11/17/%E8%AF%91-Pulsar%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">[译]-Pulsar简要介绍</a>
          </li>
        
          <li>
            <a href="/2019/11/10/%E8%AF%91-BookKeeper%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D/">[译]BookKeeper简要介绍</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Bao Qingping<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>